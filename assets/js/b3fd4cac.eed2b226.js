"use strict";(globalThis.webpackChunkai_robotics_book=globalThis.webpackChunkai_robotics_book||[]).push([[5639],{5498:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"module-4-vla/multimodal-perception","title":"Multi-Modal Perception","description":"Integrating vision, language, and proprioception for humanoid awareness","source":"@site/docs/module-4-vla/03-multimodal-perception.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/multimodal-perception","permalink":"/ai-robotics-book/docs/module-4-vla/multimodal-perception","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Multi-Modal Perception","description":"Integrating vision, language, and proprioception for humanoid awareness"},"sidebar":"tutorialSidebar","previous":{"title":"LLM Cognitive Planning","permalink":"/ai-robotics-book/docs/module-4-vla/llm-planning"},"next":{"title":"End-to-End VLA Architectures","permalink":"/ai-robotics-book/docs/module-4-vla/end-to-end-vla"}}');var i=t(4848),o=t(8453);const r={sidebar_position:3,title:"Multi-Modal Perception",description:"Integrating vision, language, and proprioception for humanoid awareness"},a="Multi-Modal Perception",c={},l=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Multi-Modal Perception Architecture",id:"multi-modal-perception-architecture",level:2},{value:"Sensor Modalities",id:"sensor-modalities",level:3},{value:"Vision-Language Models",id:"vision-language-models",level:2},{value:"Scene Description with VLM",id:"scene-description-with-vlm",level:3},{value:"Object Detection and Tracking",id:"object-detection-and-tracking",level:2},{value:"YOLO-based Object Detection",id:"yolo-based-object-detection",level:3},{value:"Sensor Fusion",id:"sensor-fusion",level:2},{value:"Multi-Modal Fusion Node",id:"multi-modal-fusion-node",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Implement VLM Integration",id:"exercise-1-implement-vlm-integration",level:3},{value:"Exercise 2: Object Tracking",id:"exercise-2-object-tracking",level:3},{value:"Exercise 3: Semantic Mapping",id:"exercise-3-semantic-mapping",level:3},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"multi-modal-perception",children:"Multi-Modal Perception"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Implement"})," vision-language models for scene understanding"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fuse"})," multiple sensor modalities for robust perception"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Design"})," object detection and tracking for manipulation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Create"})," semantic maps for robot navigation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integrate"})," perception outputs with planning systems"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Completed Chapters 1-2 of Module 4"}),"\n",(0,i.jsx)(n.li,{children:"Understanding of computer vision concepts"}),"\n",(0,i.jsx)(n.li,{children:"Familiarity with deep learning frameworks"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"multi-modal-perception-architecture",children:"Multi-Modal Perception Architecture"}),"\n",(0,i.jsx)(n.p,{children:"Humanoid robots require integration of multiple perceptual modalities:"}),"\n",(0,i.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Sensors"\n        CAM[RGB Cameras]\n        DEP[Depth Sensors]\n        IMU[IMU]\n        FT[Force/Torque]\n    end\n\n    subgraph "Feature Extraction"\n        VIS[Visual Features]\n        DEPTH[Depth Features]\n        PROP[Proprioceptive Features]\n    end\n\n    subgraph "Fusion"\n        VLM[Vision-Language Model]\n        OBJ[Object Detection]\n        SEM[Semantic Segmentation]\n    end\n\n    subgraph "Output"\n        SCENE[Scene Understanding]\n        ACT[Action Affordances]\n        MAP[Semantic Map]\n    end\n\n    CAM --\x3e VIS\n    DEP --\x3e DEPTH\n    IMU --\x3e PROP\n    FT --\x3e PROP\n    VIS --\x3e VLM\n    VIS --\x3e OBJ\n    DEPTH --\x3e OBJ\n    VLM --\x3e SCENE\n    OBJ --\x3e ACT\n    SEM --\x3e MAP\n\n    style VLM fill:#e3f2fd\n    style SCENE fill:#c8e6c9'}),"\n",(0,i.jsx)(n.h3,{id:"sensor-modalities",children:"Sensor Modalities"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Modality"}),(0,i.jsx)(n.th,{children:"Sensors"}),(0,i.jsx)(n.th,{children:"Information"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Vision"}),(0,i.jsx)(n.td,{children:"RGB cameras"}),(0,i.jsx)(n.td,{children:"Appearance, texture, objects"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Depth"}),(0,i.jsx)(n.td,{children:"Stereo, ToF, LiDAR"}),(0,i.jsx)(n.td,{children:"3D structure, distances"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Proprioception"}),(0,i.jsx)(n.td,{children:"IMU, encoders"}),(0,i.jsx)(n.td,{children:"Robot state, motion"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Tactile"}),(0,i.jsx)(n.td,{children:"Force sensors"}),(0,i.jsx)(n.td,{children:"Contact, manipulation"})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"vision-language-models",children:"Vision-Language Models"}),"\n",(0,i.jsx)(n.h3,{id:"scene-description-with-vlm",children:"Scene Description with VLM"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"Vision-Language Model for scene understanding.\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import List, Dict\n\n\n@dataclass\nclass SceneDescription:\n    \"\"\"Structured scene understanding output.\"\"\"\n    objects: List[Dict]\n    spatial_relations: List[str]\n    scene_type: str\n    suggested_actions: List[str]\n\n\nclass VisionLanguageNode(Node):\n    \"\"\"Process images with vision-language models.\"\"\"\n\n    def __init__(self):\n        super().__init__('vision_language')\n\n        self.bridge = CvBridge()\n\n        # Model (placeholder - in practice use CLIP, BLIP, LLaVA, etc.)\n        self.model = None\n        self._load_model()\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw',\n            self.image_callback, 10\n        )\n\n        # Publishers\n        self.description_pub = self.create_publisher(\n            String, '/perception/scene_description', 10\n        )\n\n        # Query from planner\n        self.query_sub = self.create_subscription(\n            String, '/perception/query',\n            self.query_callback, 10\n        )\n        self.answer_pub = self.create_publisher(\n            String, '/perception/answer', 10\n        )\n\n        # Latest image\n        self.latest_image = None\n\n        self.get_logger().info('Vision-Language node initialized')\n\n    def _load_model(self):\n        \"\"\"Load vision-language model.\"\"\"\n        # Placeholder for actual model loading\n        # In practice: CLIP, BLIP-2, LLaVA, etc.\n        self.get_logger().info('Loading VLM (placeholder)')\n\n    def image_callback(self, msg: Image):\n        \"\"\"Process incoming image.\"\"\"\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, 'rgb8')\n            self.latest_image = cv_image\n\n            # Generate scene description\n            description = self.describe_scene(cv_image)\n\n            # Publish\n            desc_msg = String()\n            desc_msg.data = self._format_description(description)\n            self.description_pub.publish(desc_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def query_callback(self, msg: String):\n        \"\"\"Answer a query about the current scene.\"\"\"\n        if self.latest_image is None:\n            return\n\n        query = msg.data\n        answer = self.answer_query(self.latest_image, query)\n\n        answer_msg = String()\n        answer_msg.data = answer\n        self.answer_pub.publish(answer_msg)\n\n    def describe_scene(self, image: np.ndarray) -> SceneDescription:\n        \"\"\"Generate scene description from image.\"\"\"\n        # Placeholder implementation\n        # In practice, use VLM to generate this\n\n        # Simulated detection results\n        objects = [\n            {'name': 'table', 'confidence': 0.95, 'bbox': [100, 200, 300, 400]},\n            {'name': 'cup', 'confidence': 0.87, 'bbox': [150, 250, 50, 50]},\n            {'name': 'chair', 'confidence': 0.92, 'bbox': [400, 300, 150, 200]},\n        ]\n\n        spatial_relations = [\n            'cup is on table',\n            'chair is near table',\n        ]\n\n        return SceneDescription(\n            objects=objects,\n            spatial_relations=spatial_relations,\n            scene_type='living_room',\n            suggested_actions=['pick up cup', 'sit on chair']\n        )\n\n    def answer_query(self, image: np.ndarray, query: str) -> str:\n        \"\"\"Answer a question about the image.\"\"\"\n        # Placeholder - use VLM for actual Q&A\n        query_lower = query.lower()\n\n        if 'cup' in query_lower:\n            return \"I see a cup on the table in the center of the image.\"\n        elif 'where' in query_lower:\n            return \"The objects are arranged on a table in what appears to be a living room.\"\n        else:\n            return \"I can see a table with a cup and a chair nearby.\"\n\n    def _format_description(self, desc: SceneDescription) -> str:\n        \"\"\"Format description as JSON string.\"\"\"\n        import json\n        return json.dumps({\n            'objects': desc.objects,\n            'spatial_relations': desc.spatial_relations,\n            'scene_type': desc.scene_type,\n            'suggested_actions': desc.suggested_actions\n        })\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VisionLanguageNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"object-detection-and-tracking",children:"Object Detection and Tracking"}),"\n",(0,i.jsx)(n.h3,{id:"yolo-based-object-detection",children:"YOLO-based Object Detection"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""Object detection node for humanoid manipulation."""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom vision_msgs.msg import Detection2DArray, Detection2D, ObjectHypothesisWithPose\nfrom geometry_msgs.msg import PoseStamped\nfrom cv_bridge import CvBridge\nimport numpy as np\nfrom typing import List, Tuple\n\n\nclass ObjectDetector(Node):\n    """Detect and track objects for manipulation."""\n\n    def __init__(self):\n        super().__init__(\'object_detector\')\n\n        self.bridge = CvBridge()\n\n        # Camera intrinsics\n        self.camera_matrix = None\n        self.depth_scale = 0.001  # mm to m\n\n        # Object classes relevant for manipulation\n        self.target_classes = [\n            \'cup\', \'bottle\', \'book\', \'remote\', \'phone\',\n            \'bowl\', \'plate\', \'spoon\', \'fork\', \'knife\'\n        ]\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/color/image_raw\',\n            self.image_callback, 10\n        )\n        self.depth_sub = self.create_subscription(\n            Image, \'/camera/depth/image_raw\',\n            self.depth_callback, 10\n        )\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, \'/camera/color/camera_info\',\n            self.camera_info_callback, 10\n        )\n\n        # Publishers\n        self.detection_pub = self.create_publisher(\n            Detection2DArray, \'/perception/detections\', 10\n        )\n        self.pose_pub = self.create_publisher(\n            PoseStamped, \'/perception/object_pose\', 10\n        )\n\n        # Latest data\n        self.latest_depth = None\n\n        self.get_logger().info(\'Object detector initialized\')\n\n    def camera_info_callback(self, msg: CameraInfo):\n        """Store camera intrinsics."""\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n\n    def depth_callback(self, msg: Image):\n        """Store latest depth image."""\n        self.latest_depth = self.bridge.imgmsg_to_cv2(msg)\n\n    def image_callback(self, msg: Image):\n        """Process image for object detection."""\n        if self.camera_matrix is None:\n            return\n\n        cv_image = self.bridge.imgmsg_to_cv2(msg, \'rgb8\')\n\n        # Run detection (placeholder)\n        detections = self.detect_objects(cv_image)\n\n        # Publish detections\n        det_msg = Detection2DArray()\n        det_msg.header = msg.header\n\n        for det in detections:\n            det_msg.detections.append(det)\n\n        self.detection_pub.publish(det_msg)\n\n        # Estimate 3D pose for first detection\n        if detections and self.latest_depth is not None:\n            pose = self.estimate_3d_pose(detections[0], self.latest_depth)\n            if pose:\n                pose.header = msg.header\n                self.pose_pub.publish(pose)\n\n    def detect_objects(self, image: np.ndarray) -> List[Detection2D]:\n        """Run object detection on image."""\n        # Placeholder - use YOLO, Detectron2, etc.\n        detections = []\n\n        # Simulated detection\n        det = Detection2D()\n        det.bbox.center.position.x = 320.0\n        det.bbox.center.position.y = 240.0\n        det.bbox.size_x = 100.0\n        det.bbox.size_y = 100.0\n\n        hyp = ObjectHypothesisWithPose()\n        hyp.hypothesis.class_id = \'cup\'\n        hyp.hypothesis.score = 0.92\n        det.results.append(hyp)\n\n        detections.append(det)\n\n        return detections\n\n    def estimate_3d_pose(self, detection: Detection2D,\n                         depth_image: np.ndarray) -> PoseStamped:\n        """Estimate 3D pose from detection and depth."""\n        # Get center pixel\n        cx = int(detection.bbox.center.position.x)\n        cy = int(detection.bbox.center.position.y)\n\n        # Get depth at center\n        if 0 <= cx < depth_image.shape[1] and 0 <= cy < depth_image.shape[0]:\n            depth = depth_image[cy, cx] * self.depth_scale\n        else:\n            return None\n\n        if depth <= 0 or depth > 10:\n            return None\n\n        # Back-project to 3D\n        fx = self.camera_matrix[0, 0]\n        fy = self.camera_matrix[1, 1]\n        cx_cam = self.camera_matrix[0, 2]\n        cy_cam = self.camera_matrix[1, 2]\n\n        x = (cx - cx_cam) * depth / fx\n        y = (cy - cy_cam) * depth / fy\n        z = depth\n\n        pose = PoseStamped()\n        pose.pose.position.x = x\n        pose.pose.position.y = y\n        pose.pose.position.z = z\n        pose.pose.orientation.w = 1.0\n\n        return pose\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ObjectDetector()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,i.jsx)(n.h3,{id:"multi-modal-fusion-node",children:"Multi-Modal Fusion Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"Fuse multi-modal perception for robust understanding.\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu, JointState\nfrom vision_msgs.msg import Detection2DArray\nfrom std_msgs.msg import String\nimport json\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, List, Optional\nimport numpy as np\n\n\n@dataclass\nclass FusedPerception:\n    \"\"\"Fused multi-modal perception output.\"\"\"\n    timestamp: float\n    detected_objects: List[Dict]\n    robot_state: Dict\n    scene_context: str\n    confidence: float\n\n\nclass PerceptionFusion(Node):\n    \"\"\"Fuse visual, depth, and proprioceptive information.\"\"\"\n\n    def __init__(self):\n        super().__init__('perception_fusion')\n\n        # Latest sensor data\n        self.latest_detections: Optional[Detection2DArray] = None\n        self.latest_imu: Optional[Imu] = None\n        self.latest_joints: Optional[JointState] = None\n        self.latest_scene: Optional[str] = None\n\n        # Subscribers\n        self.detection_sub = self.create_subscription(\n            Detection2DArray, '/perception/detections',\n            self.detection_callback, 10\n        )\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data',\n            self.imu_callback, 10\n        )\n        self.joint_sub = self.create_subscription(\n            JointState, '/joint_states',\n            self.joint_callback, 10\n        )\n        self.scene_sub = self.create_subscription(\n            String, '/perception/scene_description',\n            self.scene_callback, 10\n        )\n\n        # Publisher\n        self.fused_pub = self.create_publisher(\n            String, '/perception/fused', 10\n        )\n\n        # Fusion timer (10 Hz)\n        self.timer = self.create_timer(0.1, self.fuse_perception)\n\n        self.get_logger().info('Perception fusion initialized')\n\n    def detection_callback(self, msg: Detection2DArray):\n        self.latest_detections = msg\n\n    def imu_callback(self, msg: Imu):\n        self.latest_imu = msg\n\n    def joint_callback(self, msg: JointState):\n        self.latest_joints = msg\n\n    def scene_callback(self, msg: String):\n        self.latest_scene = msg.data\n\n    def fuse_perception(self):\n        \"\"\"Fuse all perception modalities.\"\"\"\n        # Extract object detections\n        objects = []\n        if self.latest_detections:\n            for det in self.latest_detections.detections:\n                if det.results:\n                    obj = {\n                        'class': det.results[0].hypothesis.class_id,\n                        'confidence': det.results[0].hypothesis.score,\n                        'bbox': {\n                            'x': det.bbox.center.position.x,\n                            'y': det.bbox.center.position.y,\n                            'w': det.bbox.size_x,\n                            'h': det.bbox.size_y\n                        }\n                    }\n                    objects.append(obj)\n\n        # Extract robot state\n        robot_state = {}\n        if self.latest_imu:\n            robot_state['orientation'] = {\n                'x': self.latest_imu.orientation.x,\n                'y': self.latest_imu.orientation.y,\n                'z': self.latest_imu.orientation.z,\n                'w': self.latest_imu.orientation.w\n            }\n            robot_state['angular_velocity'] = {\n                'x': self.latest_imu.angular_velocity.x,\n                'y': self.latest_imu.angular_velocity.y,\n                'z': self.latest_imu.angular_velocity.z\n            }\n\n        if self.latest_joints:\n            robot_state['joints'] = dict(zip(\n                self.latest_joints.name,\n                self.latest_joints.position\n            ))\n\n        # Parse scene context\n        scene_context = \"unknown\"\n        if self.latest_scene:\n            try:\n                scene_data = json.loads(self.latest_scene)\n                scene_context = scene_data.get('scene_type', 'unknown')\n            except json.JSONDecodeError:\n                pass\n\n        # Compute fusion confidence\n        confidence = self._compute_confidence(objects, robot_state)\n\n        # Create fused output\n        fused = FusedPerception(\n            timestamp=self.get_clock().now().nanoseconds / 1e9,\n            detected_objects=objects,\n            robot_state=robot_state,\n            scene_context=scene_context,\n            confidence=confidence\n        )\n\n        # Publish\n        msg = String()\n        msg.data = json.dumps(asdict(fused))\n        self.fused_pub.publish(msg)\n\n    def _compute_confidence(self, objects: List[Dict],\n                           robot_state: Dict) -> float:\n        \"\"\"Compute overall perception confidence.\"\"\"\n        confidence_factors = []\n\n        # Object detection confidence\n        if objects:\n            avg_det_conf = np.mean([o['confidence'] for o in objects])\n            confidence_factors.append(avg_det_conf)\n\n        # Robot state availability\n        state_completeness = 0.0\n        if 'orientation' in robot_state:\n            state_completeness += 0.5\n        if 'joints' in robot_state:\n            state_completeness += 0.5\n        confidence_factors.append(state_completeness)\n\n        if confidence_factors:\n            return float(np.mean(confidence_factors))\n        return 0.0\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PerceptionFusion()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsx)(n.h3,{id:"exercise-1-implement-vlm-integration",children:"Exercise 1: Implement VLM Integration"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate CLIP or BLIP-2 for image understanding"}),"\n",(0,i.jsx)(n.li,{children:"Test scene description generation"}),"\n",(0,i.jsx)(n.li,{children:"Evaluate accuracy on test images"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"exercise-2-object-tracking",children:"Exercise 2: Object Tracking"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Add multi-object tracking to the detector"}),"\n",(0,i.jsx)(n.li,{children:"Maintain object IDs across frames"}),"\n",(0,i.jsx)(n.li,{children:"Handle occlusions gracefully"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"exercise-3-semantic-mapping",children:"Exercise 3: Semantic Mapping"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Build a semantic map from fused perception"}),"\n",(0,i.jsx)(n.li,{children:"Store object locations in world coordinates"}),"\n",(0,i.jsx)(n.li,{children:"Query the map for planning"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What are the benefits of multi-modal perception over single-modality systems?"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"How do vision-language models improve robot understanding of environments?"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What challenges arise when fusing sensors with different update rates?"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"How would you handle conflicting information from different sensors?"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"This chapter covered multi-modal perception:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vision-language models"})," enable natural language scene understanding"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Object detection"})," identifies manipulation targets"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor fusion"})," combines modalities for robust perception"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integration"})," connects perception to planning systems"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Next, we'll explore end-to-end VLA architectures."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Next"}),": ",(0,i.jsx)(n.a,{href:"./end-to-end-vla",children:"End-to-End VLA"})]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var s=t(6540);const i={},o=s.createContext(i);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);