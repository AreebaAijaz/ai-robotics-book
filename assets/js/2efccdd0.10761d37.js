"use strict";(globalThis.webpackChunkai_robotics_book=globalThis.webpackChunkai_robotics_book||[]).push([[2680],{1500:(n,e,s)=>{s.r(e),s.d(e,{assets:()=>r,contentTitle:()=>l,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vla/llm-planning","title":"LLM Cognitive Planning","description":"Using Large Language Models for robot task planning and reasoning","source":"@site/docs/module-4-vla/02-llm-planning.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/llm-planning","permalink":"/ai-robotics-book/docs/module-4-vla/llm-planning","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"LLM Cognitive Planning","description":"Using Large Language Models for robot task planning and reasoning"},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action Pipelines","permalink":"/ai-robotics-book/docs/module-4-vla/voice-to-action"},"next":{"title":"Multi-Modal Perception","permalink":"/ai-robotics-book/docs/module-4-vla/multimodal-perception"}}');var a=s(4848),i=s(8453);const o={sidebar_position:2,title:"LLM Cognitive Planning",description:"Using Large Language Models for robot task planning and reasoning"},l="LLM Cognitive Planning",r={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"LLM Planning Architecture",id:"llm-planning-architecture",level:2},{value:"Why LLMs for Robot Planning?",id:"why-llms-for-robot-planning",level:3},{value:"Task Decomposition",id:"task-decomposition",level:2},{value:"LLM Planner Implementation",id:"llm-planner-implementation",level:3},{value:"Action Grounding",id:"action-grounding",level:2},{value:"Grounding LLM Plans to Robot Actions",id:"grounding-llm-plans-to-robot-actions",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Implement LLM API Call",id:"exercise-1-implement-llm-api-call",level:3},{value:"Exercise 2: Add Plan Validation",id:"exercise-2-add-plan-validation",level:3},{value:"Exercise 3: Implement Replanning",id:"exercise-3-implement-replanning",level:3},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"llm-cognitive-planning",children:"LLM Cognitive Planning"})}),"\n",(0,a.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Design"})," LLM-based planning systems for humanoid robots"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Implement"})," task decomposition using language models"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Ground"})," LLM outputs in robot-executable actions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Handle"})," planning failures and replanning strategies"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Evaluate"})," safety constraints in LLM-generated plans"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Completed Chapter 1: Voice-to-Action Pipelines"}),"\n",(0,a.jsx)(e.li,{children:"Understanding of transformer architectures"}),"\n",(0,a.jsx)(e.li,{children:"API access to an LLM (OpenAI, Claude, or local model)"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"llm-planning-architecture",children:"LLM Planning Architecture"}),"\n",(0,a.jsx)(e.p,{children:"LLMs enable robots to reason about complex tasks through natural language:"}),"\n",(0,a.jsx)(e.mermaid,{value:'graph TB\n    subgraph "High-Level Planning"\n        USER[User Command] --\x3e LLM[LLM Planner]\n        LLM --\x3e PLAN[Task Plan]\n        WORLD[World State] --\x3e LLM\n    end\n\n    subgraph "Grounding"\n        PLAN --\x3e VALID[Plan Validator]\n        VALID --\x3e GROUND[Action Grounder]\n        SKILLS[Robot Skills] --\x3e GROUND\n    end\n\n    subgraph "Execution"\n        GROUND --\x3e EXEC[Executor]\n        EXEC --\x3e MONITOR[Monitor]\n        MONITOR --\x3e|Failure| LLM\n    end\n\n    style LLM fill:#e3f2fd\n    style GROUND fill:#c8e6c9\n    style EXEC fill:#ffe0b2'}),"\n",(0,a.jsx)(e.h3,{id:"why-llms-for-robot-planning",children:"Why LLMs for Robot Planning?"}),"\n",(0,a.jsxs)(e.table,{children:[(0,a.jsx)(e.thead,{children:(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.th,{children:"Capability"}),(0,a.jsx)(e.th,{children:"Traditional Planning"}),(0,a.jsx)(e.th,{children:"LLM Planning"})]})}),(0,a.jsxs)(e.tbody,{children:[(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"Language understanding"}),(0,a.jsx)(e.td,{children:"Limited vocabulary"}),(0,a.jsx)(e.td,{children:"Natural language"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"Common sense"}),(0,a.jsx)(e.td,{children:"Explicit encoding"}),(0,a.jsx)(e.td,{children:"Emergent"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"Generalization"}),(0,a.jsx)(e.td,{children:"Domain-specific"}),(0,a.jsx)(e.td,{children:"Cross-domain"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"Adaptation"}),(0,a.jsx)(e.td,{children:"Requires reprogramming"}),(0,a.jsx)(e.td,{children:"Few-shot learning"})]})]})]}),"\n",(0,a.jsx)(e.h2,{id:"task-decomposition",children:"Task Decomposition"}),"\n",(0,a.jsx)(e.h3,{id:"llm-planner-implementation",children:"LLM Planner Implementation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""LLM-based task planner for humanoid robot."""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Optional\nimport os\n\n\n@dataclass\nclass PlanStep:\n    """Single step in a task plan."""\n    action: str\n    parameters: Dict\n    preconditions: List[str]\n    effects: List[str]\n    description: str\n\n\n@dataclass\nclass TaskPlan:\n    """Complete task plan."""\n    goal: str\n    steps: List[PlanStep]\n    estimated_duration: float\n\n\nclass LLMPlanner(Node):\n    """Use LLM for high-level task planning."""\n\n    def __init__(self):\n        super().__init__(\'llm_planner\')\n\n        # Robot skills (actions the robot can perform)\n        self.available_skills = {\n            \'navigate_to\': {\n                \'params\': [\'location\'],\n                \'preconditions\': [\'robot_is_standing\'],\n                \'effects\': [\'robot_at(location)\'],\n                \'description\': \'Navigate to a location\'\n            },\n            \'pick_object\': {\n                \'params\': [\'object\'],\n                \'preconditions\': [\'robot_near(object)\', \'gripper_empty\'],\n                \'effects\': [\'holding(object)\'],\n                \'description\': \'Pick up an object\'\n            },\n            \'place_object\': {\n                \'params\': [\'object\', \'surface\'],\n                \'preconditions\': [\'holding(object)\', \'robot_near(surface)\'],\n                \'effects\': [\'object_on(object, surface)\', \'gripper_empty\'],\n                \'description\': \'Place held object on surface\'\n            },\n            \'open_door\': {\n                \'params\': [\'door\'],\n                \'preconditions\': [\'robot_near(door)\', \'gripper_empty\'],\n                \'effects\': [\'door_open(door)\'],\n                \'description\': \'Open a door\'\n            },\n            \'say\': {\n                \'params\': [\'message\'],\n                \'preconditions\': [],\n                \'effects\': [],\n                \'description\': \'Speak a message\'\n            },\n            \'wait\': {\n                \'params\': [\'duration\'],\n                \'preconditions\': [],\n                \'effects\': [],\n                \'description\': \'Wait for specified seconds\'\n            }\n        }\n\n        # World state\n        self.world_state = {\n            \'robot_location\': \'living_room\',\n            \'robot_is_standing\': True,\n            \'gripper_empty\': True,\n            \'known_objects\': [\'cup\', \'book\', \'remote\'],\n            \'known_locations\': [\'kitchen\', \'living_room\', \'bedroom\'],\n            \'doors\': {\'kitchen_door\': \'closed\'}\n        }\n\n        # Subscribers and publishers\n        self.command_sub = self.create_subscription(\n            String, \'/speech/intent\',\n            self.command_callback, 10\n        )\n        self.plan_pub = self.create_publisher(\n            String, \'/planner/plan\', 10\n        )\n\n        self.get_logger().info(\'LLM planner initialized\')\n\n    def command_callback(self, msg: String):\n        """Handle incoming command."""\n        try:\n            intent = json.loads(msg.data)\n            command = intent.get(\'original_text\', \'\')\n\n            # Generate plan using LLM\n            plan = self.generate_plan(command)\n\n            if plan:\n                # Publish plan\n                plan_msg = String()\n                plan_msg.data = json.dumps({\n                    \'goal\': plan.goal,\n                    \'steps\': [\n                        {\n                            \'action\': s.action,\n                            \'parameters\': s.parameters,\n                            \'description\': s.description\n                        }\n                        for s in plan.steps\n                    ]\n                })\n                self.plan_pub.publish(plan_msg)\n\n        except json.JSONDecodeError:\n            self.get_logger().error(\'Invalid JSON in command\')\n\n    def generate_plan(self, command: str) -> Optional[TaskPlan]:\n        """Generate a plan using LLM."""\n        # Build prompt\n        prompt = self._build_prompt(command)\n\n        # Call LLM (using local implementation for this example)\n        response = self._call_llm(prompt)\n\n        # Parse response into plan\n        plan = self._parse_plan(response, command)\n\n        return plan\n\n    def _build_prompt(self, command: str) -> str:\n        """Build prompt for LLM."""\n        skills_desc = "\\n".join([\n            f"- {name}({\', \'.join(data[\'params\'])}): {data[\'description\']}"\n            for name, data in self.available_skills.items()\n        ])\n\n        state_desc = json.dumps(self.world_state, indent=2)\n\n        prompt = f"""You are a task planner for a humanoid robot. Given a user command, generate a step-by-step plan using available skills.\n\nAvailable Skills:\n{skills_desc}\n\nCurrent World State:\n{state_desc}\n\nRules:\n1. Only use available skills\n2. Ensure preconditions are met before each action\n3. Consider the current world state\n4. Break complex tasks into simple steps\n5. Output a JSON array of steps\n\nUser Command: "{command}"\n\nGenerate a plan as a JSON array where each step has:\n- "action": skill name\n- "parameters": dict of parameter values\n- "description": human-readable description\n\nPlan:"""\n\n        return prompt\n\n    def _call_llm(self, prompt: str) -> str:\n        """Call LLM API."""\n        # For this example, use a simple rule-based fallback\n        # In production, use OpenAI, Claude, or local LLM\n\n        # Simple pattern matching for demonstration\n        if \'bring\' in prompt.lower() and \'cup\' in prompt.lower():\n            return """[\n                {"action": "navigate_to", "parameters": {"location": "kitchen"}, "description": "Go to kitchen"},\n                {"action": "pick_object", "parameters": {"object": "cup"}, "description": "Pick up the cup"},\n                {"action": "navigate_to", "parameters": {"location": "living_room"}, "description": "Return to living room"},\n                {"action": "say", "parameters": {"message": "Here is your cup"}, "description": "Announce completion"}\n            ]"""\n\n        elif \'clean\' in prompt.lower() or \'tidy\' in prompt.lower():\n            return """[\n                {"action": "say", "parameters": {"message": "Starting cleanup"}, "description": "Announce start"},\n                {"action": "navigate_to", "parameters": {"location": "living_room"}, "description": "Go to living room"},\n                {"action": "pick_object", "parameters": {"object": "remote"}, "description": "Pick up remote"},\n                {"action": "place_object", "parameters": {"object": "remote", "surface": "coffee_table"}, "description": "Place remote on table"},\n                {"action": "say", "parameters": {"message": "Cleanup complete"}, "description": "Announce completion"}\n            ]"""\n\n        else:\n            return """[\n                {"action": "say", "parameters": {"message": "I understood your request but need more specific instructions"}, "description": "Request clarification"}\n            ]"""\n\n    def _parse_plan(self, response: str, goal: str) -> Optional[TaskPlan]:\n        """Parse LLM response into TaskPlan."""\n        try:\n            steps_data = json.loads(response)\n            steps = []\n\n            for step_data in steps_data:\n                action = step_data[\'action\']\n\n                # Validate action exists\n                if action not in self.available_skills:\n                    self.get_logger().warn(f\'Unknown action: {action}\')\n                    continue\n\n                skill = self.available_skills[action]\n\n                step = PlanStep(\n                    action=action,\n                    parameters=step_data.get(\'parameters\', {}),\n                    preconditions=skill[\'preconditions\'],\n                    effects=skill[\'effects\'],\n                    description=step_data.get(\'description\', \'\')\n                )\n                steps.append(step)\n\n            return TaskPlan(\n                goal=goal,\n                steps=steps,\n                estimated_duration=len(steps) * 10.0  # Rough estimate\n            )\n\n        except json.JSONDecodeError:\n            self.get_logger().error(\'Failed to parse LLM response\')\n            return None\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LLMPlanner()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"action-grounding",children:"Action Grounding"}),"\n",(0,a.jsx)(e.h3,{id:"grounding-llm-plans-to-robot-actions",children:"Grounding LLM Plans to Robot Actions"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"Ground LLM plans to executable robot actions.\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom nav2_msgs.action import NavigateToPose\nimport json\nfrom typing import Dict, Any, Callable\n\n\nclass ActionGrounder(Node):\n    \"\"\"Ground high-level actions to robot primitives.\"\"\"\n\n    def __init__(self):\n        super().__init__('action_grounder')\n\n        # Location coordinates\n        self.locations = {\n            'kitchen': {'x': 5.0, 'y': 2.0, 'yaw': 0.0},\n            'living_room': {'x': 0.0, 'y': 0.0, 'yaw': 0.0},\n            'bedroom': {'x': -3.0, 'y': 4.0, 'yaw': 1.57},\n        }\n\n        # Object locations (where objects typically are)\n        self.object_locations = {\n            'cup': 'kitchen',\n            'book': 'bedroom',\n            'remote': 'living_room',\n        }\n\n        # Action grounders\n        self.grounders: Dict[str, Callable] = {\n            'navigate_to': self.ground_navigate,\n            'pick_object': self.ground_pick,\n            'place_object': self.ground_place,\n            'say': self.ground_say,\n            'wait': self.ground_wait,\n        }\n\n        # Current plan execution\n        self.current_plan = None\n        self.current_step = 0\n\n        # Subscribers\n        self.plan_sub = self.create_subscription(\n            String, '/planner/plan',\n            self.plan_callback, 10\n        )\n\n        # Publishers\n        self.feedback_pub = self.create_publisher(\n            String, '/speech/feedback', 10\n        )\n\n        # Action clients\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n\n        self.get_logger().info('Action grounder initialized')\n\n    def plan_callback(self, msg: String):\n        \"\"\"Handle incoming plan.\"\"\"\n        try:\n            plan = json.loads(msg.data)\n            self.current_plan = plan['steps']\n            self.current_step = 0\n\n            self.get_logger().info(f\"Executing plan with {len(self.current_plan)} steps\")\n            self.execute_next_step()\n\n        except json.JSONDecodeError:\n            self.get_logger().error('Invalid plan JSON')\n\n    def execute_next_step(self):\n        \"\"\"Execute the next step in the plan.\"\"\"\n        if self.current_step >= len(self.current_plan):\n            self.get_logger().info('Plan execution complete')\n            self.publish_feedback('Task completed successfully')\n            return\n\n        step = self.current_plan[self.current_step]\n        action = step['action']\n        params = step['parameters']\n\n        self.get_logger().info(f\"Step {self.current_step + 1}: {step['description']}\")\n\n        if action in self.grounders:\n            self.grounders[action](params)\n        else:\n            self.get_logger().warn(f'No grounder for action: {action}')\n            self.advance_step()\n\n    def advance_step(self):\n        \"\"\"Move to next step.\"\"\"\n        self.current_step += 1\n        self.execute_next_step()\n\n    def ground_navigate(self, params: Dict):\n        \"\"\"Ground navigation action.\"\"\"\n        location = params.get('location')\n\n        if location not in self.locations:\n            self.get_logger().error(f'Unknown location: {location}')\n            self.advance_step()\n            return\n\n        loc = self.locations[location]\n\n        # Create navigation goal\n        goal = NavigateToPose.Goal()\n        goal.pose.header.frame_id = 'map'\n        goal.pose.header.stamp = self.get_clock().now().to_msg()\n        goal.pose.pose.position.x = loc['x']\n        goal.pose.pose.position.y = loc['y']\n\n        import numpy as np\n        goal.pose.pose.orientation.z = np.sin(loc['yaw'] / 2)\n        goal.pose.pose.orientation.w = np.cos(loc['yaw'] / 2)\n\n        self.publish_feedback(f'Navigating to {location}')\n\n        # Send goal\n        self.nav_client.wait_for_server()\n        future = self.nav_client.send_goal_async(goal)\n        future.add_done_callback(self.nav_done_callback)\n\n    def nav_done_callback(self, future):\n        \"\"\"Handle navigation completion.\"\"\"\n        goal_handle = future.result()\n        if goal_handle.accepted:\n            result_future = goal_handle.get_result_async()\n            result_future.add_done_callback(lambda f: self.advance_step())\n        else:\n            self.get_logger().error('Navigation goal rejected')\n            self.advance_step()\n\n    def ground_pick(self, params: Dict):\n        \"\"\"Ground pick action.\"\"\"\n        obj = params.get('object')\n        self.publish_feedback(f'Picking up {obj}')\n        # TODO: Implement actual manipulation\n        # For now, simulate with delay\n        self.create_timer(2.0, lambda: self.advance_step(), oneshot=True)\n\n    def ground_place(self, params: Dict):\n        \"\"\"Ground place action.\"\"\"\n        obj = params.get('object')\n        surface = params.get('surface')\n        self.publish_feedback(f'Placing {obj} on {surface}')\n        # TODO: Implement actual placement\n        self.create_timer(2.0, lambda: self.advance_step(), oneshot=True)\n\n    def ground_say(self, params: Dict):\n        \"\"\"Ground speech action.\"\"\"\n        message = params.get('message', '')\n        self.publish_feedback(message)\n        self.advance_step()\n\n    def ground_wait(self, params: Dict):\n        \"\"\"Ground wait action.\"\"\"\n        duration = float(params.get('duration', 1.0))\n        self.create_timer(duration, lambda: self.advance_step(), oneshot=True)\n\n    def publish_feedback(self, text: str):\n        \"\"\"Publish feedback message.\"\"\"\n        msg = String()\n        msg.data = text\n        self.feedback_pub.publish(msg)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ActionGrounder()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsx)(e.h3,{id:"exercise-1-implement-llm-api-call",children:"Exercise 1: Implement LLM API Call"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Connect to OpenAI or Claude API"}),"\n",(0,a.jsx)(e.li,{children:"Replace the rule-based fallback with actual LLM calls"}),"\n",(0,a.jsx)(e.li,{children:"Test with various natural language commands"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"exercise-2-add-plan-validation",children:"Exercise 2: Add Plan Validation"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Implement precondition checking"}),"\n",(0,a.jsx)(e.li,{children:"Validate that plans are executable given world state"}),"\n",(0,a.jsx)(e.li,{children:"Handle invalid plans gracefully"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"exercise-3-implement-replanning",children:"Exercise 3: Implement Replanning"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Monitor execution for failures"}),"\n",(0,a.jsx)(e.li,{children:"Trigger replanning when actions fail"}),"\n",(0,a.jsx)(e.li,{children:"Update world state based on execution results"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"What are the risks of using LLMs for robot control and how can they be mitigated?"})}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"How do you ensure LLM-generated plans are safe and executable?"})}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"What is action grounding and why is it necessary?"})}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"How would you handle LLM hallucinations in robot planning?"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"This chapter covered LLM cognitive planning:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Task decomposition"})," breaks complex commands into steps"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Action grounding"})," maps abstract actions to robot primitives"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Plan validation"})," ensures safety and executability"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Replanning"})," handles execution failures"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"Next, we'll explore multi-modal perception integration."}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Next"}),": ",(0,a.jsx)(e.a,{href:"./multimodal-perception",children:"Multi-Modal Perception"})]})]})}function p(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453:(n,e,s)=>{s.d(e,{R:()=>o,x:()=>l});var t=s(6540);const a={},i=t.createContext(a);function o(n){const e=t.useContext(i);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:o(n.components),t.createElement(i.Provider,{value:e},n.children)}}}]);