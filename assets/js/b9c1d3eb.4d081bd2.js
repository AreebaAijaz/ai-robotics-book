"use strict";(globalThis.webpackChunkai_robotics_book=globalThis.webpackChunkai_robotics_book||[]).push([[9374],{8243:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-4-vla/voice-to-action","title":"Voice-to-Action Pipelines","description":"Building speech interfaces for humanoid robot control","source":"@site/docs/module-4-vla/01-voice-to-action.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/voice-to-action","permalink":"/ai-robotics-book/docs/module-4-vla/voice-to-action","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Voice-to-Action Pipelines","description":"Building speech interfaces for humanoid robot control"},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action Models","permalink":"/ai-robotics-book/docs/category/module-4-vision-language-action-models"},"next":{"title":"LLM Cognitive Planning","permalink":"/ai-robotics-book/docs/module-4-vla/llm-planning"}}');var i=t(4848),o=t(8453);const a={sidebar_position:1,title:"Voice-to-Action Pipelines",description:"Building speech interfaces for humanoid robot control"},r="Voice-to-Action Pipelines",l={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Voice-to-Action Architecture",id:"voice-to-action-architecture",level:2},{value:"Pipeline Components",id:"pipeline-components",level:3},{value:"Speech Recognition",id:"speech-recognition",level:2},{value:"Whisper-based ASR",id:"whisper-based-asr",level:3},{value:"Natural Language Understanding",id:"natural-language-understanding",level:2},{value:"Intent Classification",id:"intent-classification",level:3},{value:"Action Execution",id:"action-execution",level:2},{value:"Command Executor",id:"command-executor",level:3},{value:"Text-to-Speech Feedback",id:"text-to-speech-feedback",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Basic Voice Interface",id:"exercise-1-basic-voice-interface",level:3},{value:"Exercise 2: Custom Intents",id:"exercise-2-custom-intents",level:3},{value:"Exercise 3: Confirmation Dialog",id:"exercise-3-confirmation-dialog",level:3},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"voice-to-action-pipelines",children:"Voice-to-Action Pipelines"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Design"})," speech recognition pipelines for robot command input"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Implement"})," natural language understanding for robot actions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Map"})," spoken commands to executable robot behaviors"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Handle"})," ambiguity and confirmation in voice interfaces"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integrate"})," speech systems with ROS 2 robot control"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Completed Modules 1-3"}),"\n",(0,i.jsx)(n.li,{children:"Basic understanding of NLP concepts"}),"\n",(0,i.jsx)(n.li,{children:"Python audio processing libraries"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"voice-to-action-architecture",children:"Voice-to-Action Architecture"}),"\n",(0,i.jsx)(n.p,{children:"A voice-to-action pipeline converts spoken natural language commands into robot actions:"}),"\n",(0,i.jsx)(n.mermaid,{value:'graph LR\n    subgraph "Speech Processing"\n        MIC[Microphone] --\x3e VAD[Voice Activity Detection]\n        VAD --\x3e ASR[Speech Recognition]\n    end\n\n    subgraph "Language Understanding"\n        ASR --\x3e NLU[Intent Classification]\n        NLU --\x3e SLOT[Slot Extraction]\n        SLOT --\x3e VALID[Validation]\n    end\n\n    subgraph "Action Execution"\n        VALID --\x3e PLAN[Action Planning]\n        PLAN --\x3e EXEC[Robot Execution]\n        EXEC --\x3e FB[Feedback]\n    end\n\n    FB --\x3e|Confirmation| MIC\n\n    style ASR fill:#e3f2fd\n    style NLU fill:#c8e6c9\n    style EXEC fill:#ffe0b2'}),"\n",(0,i.jsx)(n.h3,{id:"pipeline-components",children:"Pipeline Components"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Component"}),(0,i.jsx)(n.th,{children:"Function"}),(0,i.jsx)(n.th,{children:"Example Tools"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"VAD"}),(0,i.jsx)(n.td,{children:"Detect speech segments"}),(0,i.jsx)(n.td,{children:"WebRTC VAD, Silero"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"ASR"}),(0,i.jsx)(n.td,{children:"Transcribe audio"}),(0,i.jsx)(n.td,{children:"Whisper, Vosk"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"NLU"}),(0,i.jsx)(n.td,{children:"Extract intent"}),(0,i.jsx)(n.td,{children:"Rasa, spaCy"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Slot Filling"}),(0,i.jsx)(n.td,{children:"Extract parameters"}),(0,i.jsx)(n.td,{children:"Named Entity Recognition"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Planning"}),(0,i.jsx)(n.td,{children:"Generate actions"}),(0,i.jsx)(n.td,{children:"Behavior Trees"})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"speech-recognition",children:"Speech Recognition"}),"\n",(0,i.jsx)(n.h3,{id:"whisper-based-asr",children:"Whisper-based ASR"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"Speech recognition node using Whisper.\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nimport numpy as np\nimport whisper\nimport queue\nimport threading\n\n\nclass SpeechRecognitionNode(Node):\n    \"\"\"ROS 2 node for speech recognition.\"\"\"\n\n    def __init__(self):\n        super().__init__('speech_recognition')\n\n        # Parameters\n        self.declare_parameter('model_size', 'base')\n        self.declare_parameter('language', 'en')\n        self.declare_parameter('sample_rate', 16000)\n\n        model_size = self.get_parameter('model_size').value\n        self.language = self.get_parameter('language').value\n        self.sample_rate = self.get_parameter('sample_rate').value\n\n        # Load Whisper model\n        self.get_logger().info(f'Loading Whisper {model_size} model...')\n        self.model = whisper.load_model(model_size)\n        self.get_logger().info('Model loaded')\n\n        # Audio buffer\n        self.audio_buffer = queue.Queue()\n        self.buffer_duration = 5.0  # seconds\n        self.buffer_samples = int(self.buffer_duration * self.sample_rate)\n        self.current_buffer = np.array([], dtype=np.float32)\n\n        # Subscribers and publishers\n        self.audio_sub = self.create_subscription(\n            AudioData, '/audio/input',\n            self.audio_callback, 10\n        )\n        self.transcript_pub = self.create_publisher(\n            String, '/speech/transcript', 10\n        )\n\n        # Processing thread\n        self.processing_thread = threading.Thread(target=self._process_loop)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n\n        self.get_logger().info('Speech recognition node ready')\n\n    def audio_callback(self, msg: AudioData):\n        \"\"\"Receive audio data.\"\"\"\n        # Convert bytes to float32\n        audio_np = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n        self.audio_buffer.put(audio_np)\n\n    def _process_loop(self):\n        \"\"\"Process audio and generate transcripts.\"\"\"\n        while rclpy.ok():\n            # Collect audio chunks\n            while not self.audio_buffer.empty():\n                chunk = self.audio_buffer.get()\n                self.current_buffer = np.concatenate([self.current_buffer, chunk])\n\n            # Process when we have enough audio\n            if len(self.current_buffer) >= self.buffer_samples:\n                audio_segment = self.current_buffer[:self.buffer_samples]\n                self.current_buffer = self.current_buffer[self.buffer_samples // 2:]  # 50% overlap\n\n                # Run transcription\n                result = self.model.transcribe(\n                    audio_segment,\n                    language=self.language,\n                    fp16=False\n                )\n\n                transcript = result['text'].strip()\n                if transcript:\n                    msg = String()\n                    msg.data = transcript\n                    self.transcript_pub.publish(msg)\n                    self.get_logger().info(f'Transcript: {transcript}')\n\n    def destroy_node(self):\n        super().destroy_node()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SpeechRecognitionNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"natural-language-understanding",children:"Natural Language Understanding"}),"\n",(0,i.jsx)(n.h3,{id:"intent-classification",children:"Intent Classification"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"Intent classification for robot commands.\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\nimport re\n\n\n@dataclass\nclass Intent:\n    \"\"\"Parsed intent from natural language.\"\"\"\n    action: str\n    confidence: float\n    slots: Dict[str, str]\n\n\nclass IntentClassifier(Node):\n    \"\"\"Classify spoken commands into robot intents.\"\"\"\n\n    def __init__(self):\n        super().__init__('intent_classifier')\n\n        # Define intents with patterns\n        self.intent_patterns = {\n            'navigate': {\n                'patterns': [\n                    r'go to (?P<location>\\w+)',\n                    r'walk to (?P<location>\\w+)',\n                    r'move to (?P<location>\\w+)',\n                    r'navigate to (?P<location>\\w+)',\n                ],\n                'action': 'navigate_to_location'\n            },\n            'pick_up': {\n                'patterns': [\n                    r'pick up (?:the )?(?P<object>\\w+)',\n                    r'grab (?:the )?(?P<object>\\w+)',\n                    r'get (?:the )?(?P<object>\\w+)',\n                ],\n                'action': 'pick_object'\n            },\n            'place': {\n                'patterns': [\n                    r'put (?:the )?(?P<object>\\w+) on (?:the )?(?P<surface>\\w+)',\n                    r'place (?:the )?(?P<object>\\w+) on (?:the )?(?P<surface>\\w+)',\n                ],\n                'action': 'place_object'\n            },\n            'wave': {\n                'patterns': [\n                    r'wave',\n                    r'say hello',\n                    r'greet',\n                ],\n                'action': 'wave_gesture'\n            },\n            'stop': {\n                'patterns': [\n                    r'stop',\n                    r'halt',\n                    r'freeze',\n                    r'emergency stop',\n                ],\n                'action': 'emergency_stop'\n            },\n            'follow': {\n                'patterns': [\n                    r'follow me',\n                    r'come with me',\n                    r'follow (?P<person>\\w+)',\n                ],\n                'action': 'follow_person'\n            }\n        }\n\n        # Compile patterns\n        self.compiled_patterns = {}\n        for intent, data in self.intent_patterns.items():\n            self.compiled_patterns[intent] = {\n                'patterns': [re.compile(p, re.IGNORECASE) for p in data['patterns']],\n                'action': data['action']\n            }\n\n        # Subscribers and publishers\n        self.transcript_sub = self.create_subscription(\n            String, '/speech/transcript',\n            self.transcript_callback, 10\n        )\n        self.intent_pub = self.create_publisher(\n            String, '/speech/intent', 10\n        )\n\n        self.get_logger().info('Intent classifier ready')\n\n    def transcript_callback(self, msg: String):\n        \"\"\"Process transcript and extract intent.\"\"\"\n        text = msg.data.lower().strip()\n\n        intent = self.classify(text)\n\n        if intent:\n            # Publish intent as JSON string\n            import json\n            intent_msg = String()\n            intent_msg.data = json.dumps({\n                'action': intent.action,\n                'confidence': intent.confidence,\n                'slots': intent.slots,\n                'original_text': text\n            })\n            self.intent_pub.publish(intent_msg)\n\n            self.get_logger().info(\n                f'Intent: {intent.action}, Slots: {intent.slots}'\n            )\n        else:\n            self.get_logger().warn(f'No intent matched for: \"{text}\"')\n\n    def classify(self, text: str) -> Optional[Intent]:\n        \"\"\"Classify text into an intent.\"\"\"\n        best_match = None\n        best_confidence = 0.0\n\n        for intent_name, data in self.compiled_patterns.items():\n            for pattern in data['patterns']:\n                match = pattern.search(text)\n                if match:\n                    # Calculate simple confidence based on match coverage\n                    matched_length = match.end() - match.start()\n                    confidence = matched_length / len(text)\n\n                    if confidence > best_confidence:\n                        best_confidence = confidence\n                        slots = match.groupdict()\n                        best_match = Intent(\n                            action=data['action'],\n                            confidence=confidence,\n                            slots=slots\n                        )\n\n        return best_match\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IntentClassifier()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"action-execution",children:"Action Execution"}),"\n",(0,i.jsx)(n.h3,{id:"command-executor",children:"Command Executor"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"Execute robot actions from parsed intents.\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom nav2_msgs.action import NavigateToPose\nimport json\nfrom typing import Dict, Callable\n\n\nclass ActionExecutor(Node):\n    \"\"\"Execute robot actions based on voice commands.\"\"\"\n\n    def __init__(self):\n        super().__init__('action_executor')\n\n        # Known locations\n        self.locations = {\n            'kitchen': {'x': 5.0, 'y': 2.0, 'theta': 0.0},\n            'living_room': {'x': 0.0, 'y': 0.0, 'theta': 0.0},\n            'bedroom': {'x': -3.0, 'y': 4.0, 'theta': 1.57},\n            'office': {'x': 2.0, 'y': -3.0, 'theta': -1.57},\n        }\n\n        # Action handlers\n        self.action_handlers: Dict[str, Callable] = {\n            'navigate_to_location': self.handle_navigate,\n            'pick_object': self.handle_pick,\n            'place_object': self.handle_place,\n            'wave_gesture': self.handle_wave,\n            'emergency_stop': self.handle_stop,\n            'follow_person': self.handle_follow,\n        }\n\n        # Subscribers\n        self.intent_sub = self.create_subscription(\n            String, '/speech/intent',\n            self.intent_callback, 10\n        )\n\n        # Feedback publisher\n        self.feedback_pub = self.create_publisher(\n            String, '/speech/feedback', 10\n        )\n\n        # Action clients\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n\n        self.get_logger().info('Action executor ready')\n\n    def intent_callback(self, msg: String):\n        \"\"\"Handle incoming intent.\"\"\"\n        try:\n            intent = json.loads(msg.data)\n            action = intent['action']\n            slots = intent['slots']\n\n            if action in self.action_handlers:\n                self.action_handlers[action](slots)\n            else:\n                self.get_logger().warn(f'Unknown action: {action}')\n                self.speak_feedback(f\"I don't know how to {action}\")\n\n        except json.JSONDecodeError:\n            self.get_logger().error('Invalid intent JSON')\n\n    def handle_navigate(self, slots: Dict):\n        \"\"\"Handle navigation command.\"\"\"\n        location = slots.get('location', '').lower()\n\n        if location in self.locations:\n            loc = self.locations[location]\n            self.speak_feedback(f\"Navigating to {location}\")\n\n            # Create navigation goal\n            goal = NavigateToPose.Goal()\n            goal.pose.header.frame_id = 'map'\n            goal.pose.header.stamp = self.get_clock().now().to_msg()\n            goal.pose.pose.position.x = loc['x']\n            goal.pose.pose.position.y = loc['y']\n            goal.pose.pose.orientation.z = np.sin(loc['theta'] / 2)\n            goal.pose.pose.orientation.w = np.cos(loc['theta'] / 2)\n\n            # Send goal\n            self.nav_client.wait_for_server()\n            future = self.nav_client.send_goal_async(goal)\n            future.add_done_callback(self.nav_response_callback)\n\n        else:\n            self.speak_feedback(f\"I don't know where {location} is\")\n            self.get_logger().warn(f'Unknown location: {location}')\n\n    def handle_pick(self, slots: Dict):\n        \"\"\"Handle pick up command.\"\"\"\n        obj = slots.get('object', 'object')\n        self.speak_feedback(f\"Attempting to pick up {obj}\")\n        # TODO: Implement manipulation\n        self.get_logger().info(f'Pick object: {obj}')\n\n    def handle_place(self, slots: Dict):\n        \"\"\"Handle place command.\"\"\"\n        obj = slots.get('object', 'object')\n        surface = slots.get('surface', 'surface')\n        self.speak_feedback(f\"Placing {obj} on {surface}\")\n        # TODO: Implement placement\n        self.get_logger().info(f'Place {obj} on {surface}')\n\n    def handle_wave(self, slots: Dict):\n        \"\"\"Handle wave gesture.\"\"\"\n        self.speak_feedback(\"Waving hello!\")\n        # TODO: Trigger wave animation\n        self.get_logger().info('Wave gesture')\n\n    def handle_stop(self, slots: Dict):\n        \"\"\"Handle emergency stop.\"\"\"\n        self.speak_feedback(\"Stopping all motion\")\n        # Cancel ongoing actions\n        # TODO: Implement emergency stop\n        self.get_logger().warn('EMERGENCY STOP')\n\n    def handle_follow(self, slots: Dict):\n        \"\"\"Handle follow command.\"\"\"\n        person = slots.get('person', 'you')\n        self.speak_feedback(f\"Following {person}\")\n        # TODO: Implement person following\n        self.get_logger().info(f'Follow: {person}')\n\n    def nav_response_callback(self, future):\n        \"\"\"Handle navigation response.\"\"\"\n        goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.speak_feedback(\"Navigation goal was rejected\")\n            return\n\n        result_future = goal_handle.get_result_async()\n        result_future.add_done_callback(self.nav_result_callback)\n\n    def nav_result_callback(self, future):\n        \"\"\"Handle navigation result.\"\"\"\n        result = future.result().result\n        self.speak_feedback(\"Arrived at destination\")\n\n    def speak_feedback(self, text: str):\n        \"\"\"Publish feedback for text-to-speech.\"\"\"\n        msg = String()\n        msg.data = text\n        self.feedback_pub.publish(msg)\n        self.get_logger().info(f'Feedback: {text}')\n\n\n# Import numpy for quaternion calculation\nimport numpy as np\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ActionExecutor()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"text-to-speech-feedback",children:"Text-to-Speech Feedback"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""Text-to-speech node for robot feedback."""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport pyttsx3\nimport threading\n\n\nclass TextToSpeechNode(Node):\n    """Convert text feedback to speech."""\n\n    def __init__(self):\n        super().__init__(\'text_to_speech\')\n\n        # Initialize TTS engine\n        self.engine = pyttsx3.init()\n        self.engine.setProperty(\'rate\', 150)  # Speed\n        self.engine.setProperty(\'volume\', 0.9)\n\n        # Speech queue\n        self.speech_queue = []\n        self.speaking = False\n\n        # Subscriber\n        self.feedback_sub = self.create_subscription(\n            String, \'/speech/feedback\',\n            self.feedback_callback, 10\n        )\n\n        self.get_logger().info(\'Text-to-speech ready\')\n\n    def feedback_callback(self, msg: String):\n        """Handle feedback text."""\n        text = msg.data\n\n        # Run in separate thread to avoid blocking\n        thread = threading.Thread(target=self._speak, args=(text,))\n        thread.start()\n\n    def _speak(self, text: str):\n        """Speak the text."""\n        self.engine.say(text)\n        self.engine.runAndWait()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = TextToSpeechNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsx)(n.h3,{id:"exercise-1-basic-voice-interface",children:"Exercise 1: Basic Voice Interface"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Set up audio capture from microphone"}),"\n",(0,i.jsx)(n.li,{children:"Implement speech recognition with Whisper"}),"\n",(0,i.jsx)(n.li,{children:'Test with simple commands like "stop" and "wave"'}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"exercise-2-custom-intents",children:"Exercise 2: Custom Intents"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Add 3 new intents relevant to your application"}),"\n",(0,i.jsx)(n.li,{children:"Define slot extraction patterns"}),"\n",(0,i.jsx)(n.li,{children:"Implement action handlers for each"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"exercise-3-confirmation-dialog",children:"Exercise 3: Confirmation Dialog"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Add confirmation for critical actions (navigation)"}),"\n",(0,i.jsx)(n.li,{children:'Implement "yes/no" response handling'}),"\n",(0,i.jsx)(n.li,{children:"Test the complete dialog flow"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What are the trade-offs between on-device and cloud-based speech recognition?"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"How do you handle ambiguous commands in a voice interface?"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What safety measures should be in place for voice-controlled robots?"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"How would you implement multi-turn conversations for complex tasks?"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"This chapter covered voice-to-action pipelines:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speech recognition"})," converts audio to text using models like Whisper"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Intent classification"})," extracts actions and parameters from text"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action execution"})," maps intents to robot behaviors"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feedback"})," provides confirmation through text-to-speech"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Next, we'll explore LLM-based cognitive planning for more complex reasoning."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Next"}),": ",(0,i.jsx)(n.a,{href:"./llm-planning",children:"LLM Cognitive Planning"})]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var s=t(6540);const i={},o=s.createContext(i);function a(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);