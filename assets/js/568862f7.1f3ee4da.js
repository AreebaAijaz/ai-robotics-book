"use strict";(globalThis.webpackChunkai_robotics_book=globalThis.webpackChunkai_robotics_book||[]).push([[2747],{6661:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-3-isaac/vslam","title":"Visual SLAM","description":"Implementing Visual Simultaneous Localization and Mapping for humanoid robots","source":"@site/docs/module-3-isaac/03-vslam.md","sourceDirName":"module-3-isaac","slug":"/module-3-isaac/vslam","permalink":"/ai-robotics-book/docs/module-3-isaac/vslam","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Visual SLAM","description":"Implementing Visual Simultaneous Localization and Mapping for humanoid robots"},"sidebar":"tutorialSidebar","previous":{"title":"Isaac ROS Integration","permalink":"/ai-robotics-book/docs/module-3-isaac/isaac-ros"},"next":{"title":"Nav2 for Bipedal Robots","permalink":"/ai-robotics-book/docs/module-3-isaac/nav2-bipedal"}}');var r=a(4848),i=a(8453);const o={sidebar_position:3,title:"Visual SLAM",description:"Implementing Visual Simultaneous Localization and Mapping for humanoid robots"},t="Visual SLAM",l={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction to Visual SLAM",id:"introduction-to-visual-slam",level:2},{value:"Why Visual SLAM for Humanoids?",id:"why-visual-slam-for-humanoids",level:3},{value:"Isaac ROS Visual SLAM",id:"isaac-ros-visual-slam",level:2},{value:"Installation",id:"installation",level:3},{value:"Launch Configuration",id:"launch-configuration",level:3},{value:"Stereo Camera Setup",id:"stereo-camera-setup",level:2},{value:"Camera Configuration",id:"camera-configuration",level:3},{value:"TF Configuration",id:"tf-configuration",level:3},{value:"IMU Fusion",id:"imu-fusion",level:2},{value:"Visual-Inertial Odometry",id:"visual-inertial-odometry",level:3},{value:"Performance Evaluation",id:"performance-evaluation",level:2},{value:"SLAM Metrics",id:"slam-metrics",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Basic VSLAM Setup",id:"exercise-1-basic-vslam-setup",level:3},{value:"Exercise 2: IMU Fusion",id:"exercise-2-imu-fusion",level:3},{value:"Exercise 3: Performance Evaluation",id:"exercise-3-performance-evaluation",level:3},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Summary",id:"summary",level:2}];function m(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"visual-slam",children:"Visual SLAM"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Explain"})," the principles of Visual SLAM and its importance for humanoid navigation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Configure"})," Isaac ROS Visual SLAM for humanoid robots"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Integrate"})," stereo cameras and IMU for robust localization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Evaluate"})," SLAM performance metrics and tuning strategies"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Handle"})," challenging scenarios like dynamic environments"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Completed Chapters 1-2 of Module 3"}),"\n",(0,r.jsx)(n.li,{children:"Understanding of camera geometry and transforms"}),"\n",(0,r.jsx)(n.li,{children:"Familiarity with coordinate frames and TF"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"introduction-to-visual-slam",children:"Introduction to Visual SLAM"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Visual SLAM"})," (Simultaneous Localization and Mapping) enables robots to build a map of their environment while simultaneously tracking their position within it\u2014using only visual sensors."]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Visual SLAM Pipeline"\n        CAM[Stereo Camera] --\x3e FE[Feature Extraction]\n        IMU[IMU Data] --\x3e FE\n        FE --\x3e TRK[Feature Tracking]\n        TRK --\x3e VO[Visual Odometry]\n        VO --\x3e LC[Loop Closure]\n        LC --\x3e OPT[Graph Optimization]\n        OPT --\x3e MAP[3D Map]\n        OPT --\x3e POSE[Robot Pose]\n    end\n\n    style FE fill:#e3f2fd\n    style VO fill:#c8e6c9\n    style OPT fill:#ffe0b2'}),"\n",(0,r.jsx)(n.h3,{id:"why-visual-slam-for-humanoids",children:"Why Visual SLAM for Humanoids?"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Challenge"}),(0,r.jsx)(n.th,{children:"Visual SLAM Solution"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Dynamic terrain"}),(0,r.jsx)(n.td,{children:"Real-time pose correction"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"GPS-denied areas"}),(0,r.jsx)(n.td,{children:"Camera-based localization"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Footstep planning"}),(0,r.jsx)(n.td,{children:"Accurate height estimation"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Whole-body control"}),(0,r.jsx)(n.td,{children:"6-DOF pose tracking"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"isaac-ros-visual-slam",children:"Isaac ROS Visual SLAM"}),"\n",(0,r.jsxs)(n.p,{children:["NVIDIA provides ",(0,r.jsx)(n.strong,{children:"Isaac ROS Visual SLAM"}),", a GPU-accelerated implementation optimized for real-time performance."]}),"\n",(0,r.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Clone Isaac ROS repository\ncd ~/ros2_ws/src\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_common.git\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_visual_slam.git\n\n# Install dependencies\ncd ~/ros2_ws\nrosdep install --from-paths src --ignore-src -y\n\n# Build\ncolcon build --packages-select isaac_ros_visual_slam\nsource install/setup.bash\n"})}),"\n",(0,r.jsx)(n.h3,{id:"launch-configuration",children:"Launch Configuration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"Launch file for Isaac ROS Visual SLAM.\"\"\"\n\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\n\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Arguments\n        DeclareLaunchArgument('rectified_images', default_value='false'),\n        DeclareLaunchArgument('enable_imu_fusion', default_value='true'),\n        DeclareLaunchArgument('enable_slam_visualization', default_value='true'),\n\n        # Visual SLAM node\n        Node(\n            package='isaac_ros_visual_slam',\n            executable='isaac_ros_visual_slam_node',\n            name='visual_slam_node',\n            parameters=[{\n                'denoise_input_images': True,\n                'rectified_images': LaunchConfiguration('rectified_images'),\n                'enable_imu_fusion': LaunchConfiguration('enable_imu_fusion'),\n                'enable_slam_visualization': LaunchConfiguration('enable_slam_visualization'),\n                'enable_observations_view': True,\n                'enable_landmarks_view': True,\n\n                # Camera parameters\n                'image_jitter_threshold_ms': 35.0,\n                'sync_matching_threshold_ms': 10.0,\n\n                # IMU parameters\n                'imu_jitter_threshold_ms': 10.0,\n                'gyro_noise_density': 0.000244,\n                'gyro_random_walk': 0.000019393,\n                'accel_noise_density': 0.001862,\n                'accel_random_walk': 0.003,\n\n                # Tracking parameters\n                'publish_odom_to_base_tf': True,\n                'publish_map_to_odom_tf': True,\n                'map_frame': 'map',\n                'odom_frame': 'odom',\n                'base_frame': 'base_link',\n            }],\n            remappings=[\n                ('stereo_camera/left/image', '/camera/left/image_rect'),\n                ('stereo_camera/left/camera_info', '/camera/left/camera_info'),\n                ('stereo_camera/right/image', '/camera/right/image_rect'),\n                ('stereo_camera/right/camera_info', '/camera/right/camera_info'),\n                ('visual_slam/imu', '/imu/data'),\n            ],\n        ),\n    ])\n"})}),"\n",(0,r.jsx)(n.h2,{id:"stereo-camera-setup",children:"Stereo Camera Setup"}),"\n",(0,r.jsx)(n.h3,{id:"camera-configuration",children:"Camera Configuration"}),"\n",(0,r.jsx)(n.p,{children:"For humanoid robots, stereo cameras are typically mounted on the head:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""Stereo camera configuration for humanoid Visual SLAM."""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import CameraInfo\nimport numpy as np\n\n\nclass StereoCameraConfig(Node):\n    """Configure stereo camera for VSLAM."""\n\n    def __init__(self):\n        super().__init__(\'stereo_camera_config\')\n\n        # Camera intrinsics (typical values for 720p stereo)\n        self.image_width = 1280\n        self.image_height = 720\n        self.focal_length = 600.0  # pixels\n        self.baseline = 0.12  # 12cm between cameras\n\n        # Publishers for camera info\n        self.left_info_pub = self.create_publisher(\n            CameraInfo, \'/camera/left/camera_info\', 10\n        )\n        self.right_info_pub = self.create_publisher(\n            CameraInfo, \'/camera/right/camera_info\', 10\n        )\n\n        # Publish at 30 Hz\n        self.timer = self.create_timer(1/30.0, self.publish_camera_info)\n\n    def publish_camera_info(self):\n        """Publish stereo camera calibration."""\n        timestamp = self.get_clock().now().to_msg()\n\n        # Left camera\n        left_info = self._create_camera_info(\'left_camera_optical_frame\')\n        left_info.header.stamp = timestamp\n        self.left_info_pub.publish(left_info)\n\n        # Right camera\n        right_info = self._create_camera_info(\'right_camera_optical_frame\')\n        right_info.header.stamp = timestamp\n        # Add baseline offset in projection matrix\n        right_info.p[3] = -self.focal_length * self.baseline\n        self.right_info_pub.publish(right_info)\n\n    def _create_camera_info(self, frame_id: str) -> CameraInfo:\n        """Create CameraInfo message."""\n        msg = CameraInfo()\n        msg.header.frame_id = frame_id\n        msg.width = self.image_width\n        msg.height = self.image_height\n\n        # Intrinsic matrix K\n        cx = self.image_width / 2.0\n        cy = self.image_height / 2.0\n        msg.k = [\n            self.focal_length, 0.0, cx,\n            0.0, self.focal_length, cy,\n            0.0, 0.0, 1.0\n        ]\n\n        # No distortion (rectified images)\n        msg.d = [0.0, 0.0, 0.0, 0.0, 0.0]\n        msg.distortion_model = \'plumb_bob\'\n\n        # Rectification matrix (identity for rectified)\n        msg.r = [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]\n\n        # Projection matrix P\n        msg.p = [\n            self.focal_length, 0.0, cx, 0.0,\n            0.0, self.focal_length, cy, 0.0,\n            0.0, 0.0, 1.0, 0.0\n        ]\n\n        return msg\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = StereoCameraConfig()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"tf-configuration",children:"TF Configuration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"Static TF broadcaster for camera frames.\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom tf2_ros import StaticTransformBroadcaster\nfrom geometry_msgs.msg import TransformStamped\nimport math\n\n\nclass CameraFrameBroadcaster(Node):\n    \"\"\"Broadcast camera TF frames for VSLAM.\"\"\"\n\n    def __init__(self):\n        super().__init__('camera_frame_broadcaster')\n        self.tf_broadcaster = StaticTransformBroadcaster(self)\n        self.broadcast_transforms()\n\n    def broadcast_transforms(self):\n        \"\"\"Broadcast all camera-related transforms.\"\"\"\n        transforms = []\n\n        # Head to left camera\n        t = TransformStamped()\n        t.header.stamp = self.get_clock().now().to_msg()\n        t.header.frame_id = 'head_link'\n        t.child_frame_id = 'left_camera_link'\n        t.transform.translation.x = 0.05\n        t.transform.translation.y = 0.06  # Half baseline\n        t.transform.translation.z = 0.0\n        t.transform.rotation.w = 1.0\n        transforms.append(t)\n\n        # Left camera to left optical frame\n        t = TransformStamped()\n        t.header.stamp = self.get_clock().now().to_msg()\n        t.header.frame_id = 'left_camera_link'\n        t.child_frame_id = 'left_camera_optical_frame'\n        # Rotate to optical frame (Z forward, X right, Y down)\n        t.transform.rotation.x = -0.5\n        t.transform.rotation.y = 0.5\n        t.transform.rotation.z = -0.5\n        t.transform.rotation.w = 0.5\n        transforms.append(t)\n\n        # Head to right camera\n        t = TransformStamped()\n        t.header.stamp = self.get_clock().now().to_msg()\n        t.header.frame_id = 'head_link'\n        t.child_frame_id = 'right_camera_link'\n        t.transform.translation.x = 0.05\n        t.transform.translation.y = -0.06  # Half baseline\n        t.transform.translation.z = 0.0\n        t.transform.rotation.w = 1.0\n        transforms.append(t)\n\n        # Right camera to right optical frame\n        t = TransformStamped()\n        t.header.stamp = self.get_clock().now().to_msg()\n        t.header.frame_id = 'right_camera_link'\n        t.child_frame_id = 'right_camera_optical_frame'\n        t.transform.rotation.x = -0.5\n        t.transform.rotation.y = 0.5\n        t.transform.rotation.z = -0.5\n        t.transform.rotation.w = 0.5\n        transforms.append(t)\n\n        self.tf_broadcaster.sendTransform(transforms)\n        self.get_logger().info('Camera TF frames broadcasted')\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CameraFrameBroadcaster()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"imu-fusion",children:"IMU Fusion"}),"\n",(0,r.jsx)(n.h3,{id:"visual-inertial-odometry",children:"Visual-Inertial Odometry"}),"\n",(0,r.jsx)(n.p,{children:"Combining visual and inertial data improves robustness:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""IMU preprocessing for Visual SLAM."""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nimport numpy as np\nfrom collections import deque\n\n\nclass IMUPreprocessor(Node):\n    """Preprocess IMU data for VSLAM fusion."""\n\n    def __init__(self):\n        super().__init__(\'imu_preprocessor\')\n\n        # IMU noise parameters\n        self.gyro_noise_density = 0.000244  # rad/s/sqrt(Hz)\n        self.accel_noise_density = 0.001862  # m/s^2/sqrt(Hz)\n\n        # Bias estimation\n        self.gyro_bias = np.zeros(3)\n        self.accel_bias = np.zeros(3)\n        self.calibrated = False\n        self.calibration_samples = deque(maxlen=1000)\n\n        # Subscriber and publisher\n        self.imu_sub = self.create_subscription(\n            Imu, \'/imu/raw\', self.imu_callback, 10\n        )\n        self.imu_pub = self.create_publisher(\n            Imu, \'/imu/data\', 10\n        )\n\n        self.get_logger().info(\'IMU preprocessor initialized\')\n\n    def imu_callback(self, msg: Imu):\n        """Process raw IMU data."""\n        # Collect calibration samples if robot is stationary\n        if not self.calibrated:\n            self.calibration_samples.append({\n                \'gyro\': np.array([\n                    msg.angular_velocity.x,\n                    msg.angular_velocity.y,\n                    msg.angular_velocity.z\n                ]),\n                \'accel\': np.array([\n                    msg.linear_acceleration.x,\n                    msg.linear_acceleration.y,\n                    msg.linear_acceleration.z\n                ])\n            })\n\n            if len(self.calibration_samples) >= 1000:\n                self._estimate_bias()\n                self.calibrated = True\n\n            return\n\n        # Apply bias correction\n        corrected_msg = Imu()\n        corrected_msg.header = msg.header\n\n        corrected_msg.angular_velocity.x = msg.angular_velocity.x - self.gyro_bias[0]\n        corrected_msg.angular_velocity.y = msg.angular_velocity.y - self.gyro_bias[1]\n        corrected_msg.angular_velocity.z = msg.angular_velocity.z - self.gyro_bias[2]\n\n        corrected_msg.linear_acceleration.x = msg.linear_acceleration.x - self.accel_bias[0]\n        corrected_msg.linear_acceleration.y = msg.linear_acceleration.y - self.accel_bias[1]\n        corrected_msg.linear_acceleration.z = msg.linear_acceleration.z - self.accel_bias[2]\n\n        # Copy orientation if available\n        corrected_msg.orientation = msg.orientation\n\n        # Set covariance matrices\n        gyro_cov = self.gyro_noise_density ** 2\n        accel_cov = self.accel_noise_density ** 2\n\n        corrected_msg.angular_velocity_covariance = [\n            gyro_cov, 0, 0, 0, gyro_cov, 0, 0, 0, gyro_cov\n        ]\n        corrected_msg.linear_acceleration_covariance = [\n            accel_cov, 0, 0, 0, accel_cov, 0, 0, 0, accel_cov\n        ]\n\n        self.imu_pub.publish(corrected_msg)\n\n    def _estimate_bias(self):\n        """Estimate IMU biases from stationary data."""\n        gyro_samples = np.array([s[\'gyro\'] for s in self.calibration_samples])\n        accel_samples = np.array([s[\'accel\'] for s in self.calibration_samples])\n\n        self.gyro_bias = np.mean(gyro_samples, axis=0)\n\n        # Accel bias (assuming Z-up, subtract gravity)\n        accel_mean = np.mean(accel_samples, axis=0)\n        self.accel_bias = accel_mean - np.array([0, 0, 9.81])\n\n        self.get_logger().info(f\'Gyro bias: {self.gyro_bias}\')\n        self.get_logger().info(f\'Accel bias: {self.accel_bias}\')\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IMUPreprocessor()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"performance-evaluation",children:"Performance Evaluation"}),"\n",(0,r.jsx)(n.h3,{id:"slam-metrics",children:"SLAM Metrics"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""Visual SLAM performance evaluation."""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom nav_msgs.msg import Odometry\nfrom geometry_msgs.msg import PoseStamped\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import List\n\n\n@dataclass\nclass PoseError:\n    """Pose estimation error metrics."""\n    position_error: float  # meters\n    orientation_error: float  # radians\n    timestamp: float\n\n\nclass SLAMEvaluator(Node):\n    """Evaluate SLAM performance against ground truth."""\n\n    def __init__(self):\n        super().__init__(\'slam_evaluator\')\n\n        self.ground_truth_poses: List[PoseStamped] = []\n        self.estimated_poses: List[PoseStamped] = []\n        self.errors: List[PoseError] = []\n\n        # Subscribers\n        self.gt_sub = self.create_subscription(\n            PoseStamped, \'/ground_truth/pose\',\n            self.gt_callback, 10\n        )\n        self.slam_sub = self.create_subscription(\n            Odometry, \'/visual_slam/odom\',\n            self.slam_callback, 10\n        )\n\n        # Evaluation timer (1 Hz)\n        self.timer = self.create_timer(1.0, self.evaluate)\n\n    def gt_callback(self, msg: PoseStamped):\n        """Store ground truth pose."""\n        self.ground_truth_poses.append(msg)\n        # Keep last 1000 poses\n        if len(self.ground_truth_poses) > 1000:\n            self.ground_truth_poses.pop(0)\n\n    def slam_callback(self, msg: Odometry):\n        """Store SLAM estimated pose."""\n        pose = PoseStamped()\n        pose.header = msg.header\n        pose.pose = msg.pose.pose\n        self.estimated_poses.append(pose)\n        if len(self.estimated_poses) > 1000:\n            self.estimated_poses.pop(0)\n\n    def evaluate(self):\n        """Compute error metrics."""\n        if len(self.ground_truth_poses) < 10 or len(self.estimated_poses) < 10:\n            return\n\n        # Find closest timestamp pairs\n        errors = []\n        for est in self.estimated_poses[-100:]:\n            closest_gt = min(\n                self.ground_truth_poses,\n                key=lambda gt: abs(\n                    gt.header.stamp.sec + gt.header.stamp.nanosec * 1e-9 -\n                    est.header.stamp.sec - est.header.stamp.nanosec * 1e-9\n                )\n            )\n\n            # Position error\n            pos_error = np.sqrt(\n                (est.pose.position.x - closest_gt.pose.position.x) ** 2 +\n                (est.pose.position.y - closest_gt.pose.position.y) ** 2 +\n                (est.pose.position.z - closest_gt.pose.position.z) ** 2\n            )\n\n            errors.append(pos_error)\n\n        if errors:\n            mean_error = np.mean(errors)\n            max_error = np.max(errors)\n            self.get_logger().info(\n                f\'SLAM Error - Mean: {mean_error:.3f}m, Max: {max_error:.3f}m\'\n            )\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SLAMEvaluator()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-1-basic-vslam-setup",children:"Exercise 1: Basic VSLAM Setup"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Configure stereo camera in Isaac Sim"}),"\n",(0,r.jsx)(n.li,{children:"Launch Isaac ROS Visual SLAM"}),"\n",(0,r.jsx)(n.li,{children:"Visualize trajectory in RViz"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-2-imu-fusion",children:"Exercise 2: IMU Fusion"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Add IMU sensor to your robot"}),"\n",(0,r.jsx)(n.li,{children:"Configure IMU preprocessing"}),"\n",(0,r.jsx)(n.li,{children:"Compare VSLAM with and without IMU fusion"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-3-performance-evaluation",children:"Exercise 3: Performance Evaluation"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Record ground truth from simulation"}),"\n",(0,r.jsx)(n.li,{children:"Run SLAM evaluator during navigation"}),"\n",(0,r.jsx)(n.li,{children:"Analyze error statistics"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Why is stereo vision preferred over monocular for humanoid SLAM?"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"How does IMU fusion improve Visual SLAM robustness?"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"What are the failure modes of Visual SLAM and how can they be mitigated?"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"How would you tune VSLAM parameters for a fast-moving humanoid?"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"This chapter covered Visual SLAM for humanoid robots:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Visual SLAM"})," enables localization without GPS using cameras"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stereo cameras"})," provide depth information for accurate mapping"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"IMU fusion"})," improves robustness during fast motion"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance evaluation"})," validates SLAM accuracy"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Next, we'll explore Nav2 adaptation for bipedal locomotion."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Next"}),": ",(0,r.jsx)(n.a,{href:"./nav2-bipedal",children:"Nav2 for Bipedal Robots"})]})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(m,{...e})}):m(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>t});var s=a(6540);const r={},i=s.createContext(r);function o(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);