"use strict";(globalThis.webpackChunkai_robotics_book=globalThis.webpackChunkai_robotics_book||[]).push([[7668],{8453:(n,e,o)=>{o.d(e,{R:()=>r,x:()=>a});var t=o(6540);const s={},i=t.createContext(s);function r(n){const e=t.useContext(i);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:r(n.components),t.createElement(i.Provider,{value:e},n.children)}},8985:(n,e,o)=>{o.r(e),o.d(e,{assets:()=>d,contentTitle:()=>a,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-4-vla/end-to-end-vla","title":"End-to-End VLA Architectures","description":"Building complete Vision-Language-Action models for humanoid control","source":"@site/docs/module-4-vla/04-end-to-end-vla.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/end-to-end-vla","permalink":"/ai-robotics-book/docs/module-4-vla/end-to-end-vla","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"End-to-End VLA Architectures","description":"Building complete Vision-Language-Action models for humanoid control"},"sidebar":"tutorialSidebar","previous":{"title":"Multi-Modal Perception","permalink":"/ai-robotics-book/docs/module-4-vla/multimodal-perception"},"next":{"title":"Capstone Project","permalink":"/ai-robotics-book/docs/category/capstone-project"}}');var s=o(4848),i=o(8453);const r={sidebar_position:4,title:"End-to-End VLA Architectures",description:"Building complete Vision-Language-Action models for humanoid control"},a="End-to-End VLA Architectures",d={},l=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"VLA Architecture Overview",id:"vla-architecture-overview",level:2},{value:"Key Components",id:"key-components",level:3},{value:"VLA Model Implementation",id:"vla-model-implementation",level:2},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Training Pipeline",id:"training-pipeline",level:2},{value:"Data Collection and Training",id:"data-collection-and-training",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Implement VLA Inference Node",id:"exercise-1-implement-vla-inference-node",level:3},{value:"Exercise 2: Data Collection Pipeline",id:"exercise-2-data-collection-pipeline",level:3},{value:"Exercise 3: Evaluate VLA Performance",id:"exercise-3-evaluate-vla-performance",level:3},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Summary",id:"summary",level:2}];function c(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"end-to-end-vla-architectures",children:"End-to-End VLA Architectures"})}),"\n",(0,s.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,s.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Design"})," end-to-end VLA architectures for humanoid robots"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Implement"})," action prediction from visual and language inputs"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Train"})," VLA models using demonstration data"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Deploy"})," VLA models on robot hardware"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Evaluate"})," VLA performance and safety"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Completed Chapters 1-3 of Module 4"}),"\n",(0,s.jsx)(e.li,{children:"Deep learning framework experience (PyTorch)"}),"\n",(0,s.jsx)(e.li,{children:"Understanding of transformer architectures"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"vla-architecture-overview",children:"VLA Architecture Overview"}),"\n",(0,s.jsx)(e.p,{children:"Vision-Language-Action (VLA) models directly map perceptual inputs and language commands to robot actions:"}),"\n",(0,s.jsx)(e.mermaid,{value:'graph TB\n    subgraph "Inputs"\n        IMG[Image Sequence]\n        LANG[Language Command]\n        PROP[Proprioception]\n    end\n\n    subgraph "VLA Model"\n        VE[Vision Encoder]\n        LE[Language Encoder]\n        PE[Proprioception Encoder]\n        FUSE[Cross-Modal Fusion]\n        TRANS[Transformer Decoder]\n        HEAD[Action Head]\n    end\n\n    subgraph "Output"\n        ACT[Action Tokens]\n        JOINTS[Joint Commands]\n    end\n\n    IMG --\x3e VE\n    LANG --\x3e LE\n    PROP --\x3e PE\n    VE --\x3e FUSE\n    LE --\x3e FUSE\n    PE --\x3e FUSE\n    FUSE --\x3e TRANS\n    TRANS --\x3e HEAD\n    HEAD --\x3e ACT\n    ACT --\x3e JOINTS\n\n    style FUSE fill:#e3f2fd\n    style TRANS fill:#c8e6c9\n    style HEAD fill:#ffe0b2'}),"\n",(0,s.jsx)(e.h3,{id:"key-components",children:"Key Components"}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"Component"}),(0,s.jsx)(e.th,{children:"Function"}),(0,s.jsx)(e.th,{children:"Architecture"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Vision Encoder"}),(0,s.jsx)(e.td,{children:"Extract visual features"}),(0,s.jsx)(e.td,{children:"ViT, ResNet"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Language Encoder"}),(0,s.jsx)(e.td,{children:"Encode commands"}),(0,s.jsx)(e.td,{children:"BERT, T5"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Cross-Modal Fusion"}),(0,s.jsx)(e.td,{children:"Combine modalities"}),(0,s.jsx)(e.td,{children:"Cross-attention"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Action Decoder"}),(0,s.jsx)(e.td,{children:"Generate actions"}),(0,s.jsx)(e.td,{children:"Transformer decoder"})]})]})]}),"\n",(0,s.jsx)(e.h2,{id:"vla-model-implementation",children:"VLA Model Implementation"}),"\n",(0,s.jsx)(e.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""End-to-end VLA model for humanoid control."""\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom typing import Tuple, Optional\nimport numpy as np\n\n\nclass VisionEncoder(nn.Module):\n    """Encode visual observations."""\n\n    def __init__(self, img_size: int = 224, patch_size: int = 16,\n                 embed_dim: int = 768, num_layers: int = 12):\n        super().__init__()\n\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n\n        # Patch embedding\n        self.patch_embed = nn.Conv2d(\n            3, embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n\n        # Position embedding\n        self.pos_embed = nn.Parameter(\n            torch.zeros(1, self.num_patches + 1, embed_dim)\n        )\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n\n        # Transformer encoder\n        encoder_layer = TransformerEncoderLayer(\n            d_model=embed_dim, nhead=12, dim_feedforward=3072, batch_first=True\n        )\n        self.encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """\n        Args:\n            x: (B, C, H, W) image tensor\n        Returns:\n            (B, num_patches + 1, embed_dim) visual features\n        """\n        B = x.shape[0]\n\n        # Patch embedding\n        x = self.patch_embed(x)  # (B, embed_dim, H\', W\')\n        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n\n        # Add CLS token\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat([cls_tokens, x], dim=1)\n\n        # Add position embedding\n        x = x + self.pos_embed\n\n        # Transformer encoder\n        x = self.encoder(x)\n        x = self.norm(x)\n\n        return x\n\n\nclass LanguageEncoder(nn.Module):\n    """Encode language commands."""\n\n    def __init__(self, vocab_size: int = 32000, embed_dim: int = 768,\n                 max_seq_len: int = 64, num_layers: int = 6):\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.pos_embed = nn.Parameter(torch.zeros(1, max_seq_len, embed_dim))\n\n        encoder_layer = TransformerEncoderLayer(\n            d_model=embed_dim, nhead=12, dim_feedforward=3072, batch_first=True\n        )\n        self.encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, tokens: torch.Tensor,\n                attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        """\n        Args:\n            tokens: (B, seq_len) token indices\n            attention_mask: (B, seq_len) mask tensor\n        Returns:\n            (B, seq_len, embed_dim) language features\n        """\n        x = self.embedding(tokens)\n        x = x + self.pos_embed[:, :x.shape[1], :]\n\n        if attention_mask is not None:\n            # Convert mask for transformer (True = ignore)\n            mask = ~attention_mask.bool()\n        else:\n            mask = None\n\n        x = self.encoder(x, src_key_padding_mask=mask)\n        x = self.norm(x)\n\n        return x\n\n\nclass ProprioceptionEncoder(nn.Module):\n    """Encode robot proprioception."""\n\n    def __init__(self, num_joints: int = 24, embed_dim: int = 768):\n        super().__init__()\n\n        # Joint positions, velocities, torques\n        input_dim = num_joints * 3\n\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.Linear(512, embed_dim),\n        )\n\n    def forward(self, joint_state: torch.Tensor) -> torch.Tensor:\n        """\n        Args:\n            joint_state: (B, num_joints * 3) positions, velocities, torques\n        Returns:\n            (B, 1, embed_dim) proprioception features\n        """\n        x = self.encoder(joint_state)\n        return x.unsqueeze(1)\n\n\nclass CrossModalFusion(nn.Module):\n    """Fuse vision, language, and proprioception."""\n\n    def __init__(self, embed_dim: int = 768, num_layers: int = 4):\n        super().__init__()\n\n        self.cross_attention = nn.MultiheadAttention(\n            embed_dim, num_heads=12, batch_first=True\n        )\n\n        encoder_layer = TransformerEncoderLayer(\n            d_model=embed_dim, nhead=12, dim_feedforward=3072, batch_first=True\n        )\n        self.fusion_encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        # Modality type embeddings\n        self.vision_type = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.language_type = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.proprio_type = nn.Parameter(torch.zeros(1, 1, embed_dim))\n\n    def forward(self, vision_feat: torch.Tensor, language_feat: torch.Tensor,\n                proprio_feat: torch.Tensor) -> torch.Tensor:\n        """\n        Args:\n            vision_feat: (B, V, D) visual features\n            language_feat: (B, L, D) language features\n            proprio_feat: (B, 1, D) proprioception features\n        Returns:\n            (B, V+L+1, D) fused features\n        """\n        B = vision_feat.shape[0]\n\n        # Add modality type embeddings\n        vision_feat = vision_feat + self.vision_type\n        language_feat = language_feat + self.language_type\n        proprio_feat = proprio_feat + self.proprio_type\n\n        # Concatenate all modalities\n        fused = torch.cat([vision_feat, language_feat, proprio_feat], dim=1)\n\n        # Self-attention fusion\n        fused = self.fusion_encoder(fused)\n\n        return fused\n\n\nclass ActionDecoder(nn.Module):\n    """Decode actions from fused features."""\n\n    def __init__(self, embed_dim: int = 768, num_joints: int = 24,\n                 action_horizon: int = 16, num_layers: int = 4):\n        super().__init__()\n\n        self.action_horizon = action_horizon\n        self.num_joints = num_joints\n\n        # Learned action queries\n        self.action_queries = nn.Parameter(\n            torch.zeros(1, action_horizon, embed_dim)\n        )\n\n        # Cross-attention to fused features\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=embed_dim, nhead=12, dim_feedforward=3072, batch_first=True\n        )\n        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n\n        # Action prediction head\n        self.action_head = nn.Sequential(\n            nn.Linear(embed_dim, 512),\n            nn.ReLU(),\n            nn.Linear(512, num_joints),\n            nn.Tanh(),  # Normalize to [-1, 1]\n        )\n\n    def forward(self, fused_features: torch.Tensor) -> torch.Tensor:\n        """\n        Args:\n            fused_features: (B, S, D) fused multi-modal features\n        Returns:\n            (B, action_horizon, num_joints) predicted actions\n        """\n        B = fused_features.shape[0]\n\n        # Expand action queries\n        queries = self.action_queries.expand(B, -1, -1)\n\n        # Decode actions\n        decoded = self.decoder(queries, fused_features)\n\n        # Predict joint actions\n        actions = self.action_head(decoded)\n\n        return actions\n\n\nclass VLAModel(nn.Module):\n    """Complete Vision-Language-Action model."""\n\n    def __init__(self, num_joints: int = 24, action_horizon: int = 16,\n                 embed_dim: int = 768):\n        super().__init__()\n\n        self.vision_encoder = VisionEncoder(embed_dim=embed_dim)\n        self.language_encoder = LanguageEncoder(embed_dim=embed_dim)\n        self.proprio_encoder = ProprioceptionEncoder(\n            num_joints=num_joints, embed_dim=embed_dim\n        )\n        self.fusion = CrossModalFusion(embed_dim=embed_dim)\n        self.action_decoder = ActionDecoder(\n            embed_dim=embed_dim, num_joints=num_joints,\n            action_horizon=action_horizon\n        )\n\n        # Action scaling (learned)\n        self.action_scale = nn.Parameter(torch.ones(num_joints))\n        self.action_bias = nn.Parameter(torch.zeros(num_joints))\n\n    def forward(self, image: torch.Tensor, tokens: torch.Tensor,\n                joint_state: torch.Tensor,\n                attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        """\n        Args:\n            image: (B, C, H, W) RGB image\n            tokens: (B, L) language tokens\n            joint_state: (B, num_joints * 3) current joint state\n            attention_mask: (B, L) language attention mask\n        Returns:\n            (B, action_horizon, num_joints) predicted joint actions\n        """\n        # Encode each modality\n        vision_feat = self.vision_encoder(image)\n        language_feat = self.language_encoder(tokens, attention_mask)\n        proprio_feat = self.proprio_encoder(joint_state)\n\n        # Fuse modalities\n        fused = self.fusion(vision_feat, language_feat, proprio_feat)\n\n        # Decode actions\n        actions = self.action_decoder(fused)\n\n        # Scale actions to joint range\n        actions = actions * self.action_scale + self.action_bias\n\n        return actions\n\n\n# Example usage\ndef test_vla_model():\n    """Test VLA model forward pass."""\n    model = VLAModel(num_joints=24, action_horizon=16)\n\n    # Dummy inputs\n    batch_size = 4\n    image = torch.randn(batch_size, 3, 224, 224)\n    tokens = torch.randint(0, 32000, (batch_size, 32))\n    joint_state = torch.randn(batch_size, 24 * 3)\n\n    # Forward pass\n    actions = model(image, tokens, joint_state)\n\n    print(f"Input image shape: {image.shape}")\n    print(f"Input tokens shape: {tokens.shape}")\n    print(f"Input joint state shape: {joint_state.shape}")\n    print(f"Output actions shape: {actions.shape}")\n\n    return actions\n\n\nif __name__ == \'__main__\':\n    test_vla_model()\n'})}),"\n",(0,s.jsx)(e.h2,{id:"training-pipeline",children:"Training Pipeline"}),"\n",(0,s.jsx)(e.h3,{id:"data-collection-and-training",children:"Data Collection and Training"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""Training pipeline for VLA model."""\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport numpy as np\nfrom pathlib import Path\nfrom typing import Dict, List\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass DemoTrajectory:\n    """Single demonstration trajectory."""\n    images: np.ndarray  # (T, H, W, C)\n    language: str\n    joint_states: np.ndarray  # (T, num_joints * 3)\n    actions: np.ndarray  # (T, num_joints)\n\n\nclass VLADataset(Dataset):\n    """Dataset for VLA training."""\n\n    def __init__(self, demo_paths: List[Path], tokenizer,\n                 action_horizon: int = 16):\n        self.demos = self._load_demos(demo_paths)\n        self.tokenizer = tokenizer\n        self.action_horizon = action_horizon\n\n    def _load_demos(self, paths: List[Path]) -> List[DemoTrajectory]:\n        """Load demonstration data."""\n        demos = []\n        for path in paths:\n            # Load from files (npz, hdf5, etc.)\n            # Placeholder implementation\n            pass\n        return demos\n\n    def __len__(self):\n        return len(self.demos)\n\n    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\n        demo = self.demos[idx]\n\n        # Sample a random timestep\n        max_t = len(demo.images) - self.action_horizon\n        t = np.random.randint(0, max(1, max_t))\n\n        # Get current observation\n        image = torch.from_numpy(demo.images[t]).permute(2, 0, 1).float() / 255.0\n        joint_state = torch.from_numpy(demo.joint_states[t]).float()\n\n        # Tokenize language\n        tokens = self.tokenizer.encode(demo.language, max_length=64)\n        tokens = torch.tensor(tokens)\n\n        # Get action sequence\n        end_t = min(t + self.action_horizon, len(demo.actions))\n        actions = torch.from_numpy(demo.actions[t:end_t]).float()\n\n        # Pad if necessary\n        if actions.shape[0] < self.action_horizon:\n            pad_size = self.action_horizon - actions.shape[0]\n            actions = torch.cat([\n                actions,\n                actions[-1:].repeat(pad_size, 1)\n            ])\n\n        return {\n            \'image\': image,\n            \'tokens\': tokens,\n            \'joint_state\': joint_state,\n            \'actions\': actions\n        }\n\n\nclass VLATrainer:\n    """Training loop for VLA model."""\n\n    def __init__(self, model: nn.Module, train_dataset: Dataset,\n                 val_dataset: Dataset, config: Dict):\n        self.model = model\n        self.config = config\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n        self.model.to(self.device)\n\n        # Data loaders\n        self.train_loader = DataLoader(\n            train_dataset,\n            batch_size=config[\'batch_size\'],\n            shuffle=True,\n            num_workers=4\n        )\n        self.val_loader = DataLoader(\n            val_dataset,\n            batch_size=config[\'batch_size\'],\n            shuffle=False,\n            num_workers=4\n        )\n\n        # Optimizer\n        self.optimizer = AdamW(\n            model.parameters(),\n            lr=config[\'learning_rate\'],\n            weight_decay=config[\'weight_decay\']\n        )\n\n        # Scheduler\n        self.scheduler = CosineAnnealingLR(\n            self.optimizer,\n            T_max=config[\'num_epochs\']\n        )\n\n        # Loss function\n        self.criterion = nn.MSELoss()\n\n    def train_epoch(self) -> float:\n        """Train for one epoch."""\n        self.model.train()\n        total_loss = 0\n\n        for batch in self.train_loader:\n            # Move to device\n            image = batch[\'image\'].to(self.device)\n            tokens = batch[\'tokens\'].to(self.device)\n            joint_state = batch[\'joint_state\'].to(self.device)\n            actions_gt = batch[\'actions\'].to(self.device)\n\n            # Forward pass\n            self.optimizer.zero_grad()\n            actions_pred = self.model(image, tokens, joint_state)\n\n            # Compute loss\n            loss = self.criterion(actions_pred, actions_gt)\n\n            # Backward pass\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            self.optimizer.step()\n\n            total_loss += loss.item()\n\n        return total_loss / len(self.train_loader)\n\n    def validate(self) -> float:\n        """Validate model."""\n        self.model.eval()\n        total_loss = 0\n\n        with torch.no_grad():\n            for batch in self.val_loader:\n                image = batch[\'image\'].to(self.device)\n                tokens = batch[\'tokens\'].to(self.device)\n                joint_state = batch[\'joint_state\'].to(self.device)\n                actions_gt = batch[\'actions\'].to(self.device)\n\n                actions_pred = self.model(image, tokens, joint_state)\n                loss = self.criterion(actions_pred, actions_gt)\n                total_loss += loss.item()\n\n        return total_loss / len(self.val_loader)\n\n    def train(self):\n        """Full training loop."""\n        best_val_loss = float(\'inf\')\n\n        for epoch in range(self.config[\'num_epochs\']):\n            train_loss = self.train_epoch()\n            val_loss = self.validate()\n\n            self.scheduler.step()\n\n            print(f"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, "\n                  f"Val Loss = {val_loss:.4f}")\n\n            # Save best model\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                torch.save(self.model.state_dict(), \'best_vla_model.pt\')\n'})}),"\n",(0,s.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsx)(e.h3,{id:"exercise-1-implement-vla-inference-node",children:"Exercise 1: Implement VLA Inference Node"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Create a ROS 2 node that runs VLA inference"}),"\n",(0,s.jsx)(e.li,{children:"Subscribe to camera and joint state topics"}),"\n",(0,s.jsx)(e.li,{children:"Publish predicted actions to robot"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"exercise-2-data-collection-pipeline",children:"Exercise 2: Data Collection Pipeline"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Build a teleoperation interface for demonstrations"}),"\n",(0,s.jsx)(e.li,{children:"Record synchronized images, language, and actions"}),"\n",(0,s.jsx)(e.li,{children:"Create a dataset for training"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"exercise-3-evaluate-vla-performance",children:"Exercise 3: Evaluate VLA Performance"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Define success metrics for manipulation tasks"}),"\n",(0,s.jsx)(e.li,{children:"Evaluate on held-out test scenarios"}),"\n",(0,s.jsx)(e.li,{children:"Analyze failure cases and improvements"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"What are the advantages of end-to-end VLA models over modular approaches?"})}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"How do you ensure safety when deploying VLA models on real robots?"})}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"What is action chunking and why is it important for VLA models?"})}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"How would you handle distribution shift between training and deployment?"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(e.p,{children:"This chapter covered end-to-end VLA architectures:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multi-modal encoders"})," process vision, language, and proprioception"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Cross-modal fusion"})," combines information from all modalities"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Action decoders"})," predict future joint commands"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Training pipelines"})," learn from demonstration data"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"This completes Module 4 on Vision-Language-Action Models. Next, we'll bring everything together in the Capstone Project."}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Next Module"}),": ",(0,s.jsx)(e.a,{href:"../capstone/",children:"Capstone Project"})]})]})}function m(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(c,{...n})}):c(n)}}}]);