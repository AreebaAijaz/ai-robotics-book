"use strict";(globalThis.webpackChunkai_robotics_book=globalThis.webpackChunkai_robotics_book||[]).push([[2329],{3990:o=>{o.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/ai-robotics-book/docs/intro","label":"Introduction","docId":"intro","unlisted":false},{"type":"category","label":"Module 1: ROS 2 Fundamentals","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-robotics-book/docs/module-1-ros2/architecture","label":"ROS 2 Architecture","docId":"module-1-ros2/architecture","unlisted":false},{"type":"link","href":"/ai-robotics-book/docs/module-1-ros2/nodes-topics-services","label":"Nodes, Topics, and Services","docId":"module-1-ros2/nodes-topics-services","unlisted":false},{"type":"link","href":"/ai-robotics-book/docs/module-1-ros2/rclpy-integration","label":"Python Development with rclpy","docId":"module-1-ros2/rclpy-integration","unlisted":false},{"type":"link","href":"/ai-robotics-book/docs/module-1-ros2/urdf-humanoids","label":"URDF for Humanoid Robots","docId":"module-1-ros2/urdf-humanoids","unlisted":false}],"href":"/ai-robotics-book/docs/category/module-1-ros-2-fundamentals"},{"type":"category","label":"Module 2: Simulation Environments","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-robotics-book/docs/module-2-simulation/gazebo-basics","label":"Gazebo Basics","docId":"module-2-simulation/gazebo-basics","unlisted":false},{"type":"link","href":"/ai-robotics-book/docs/module-2-simulation/unity-integration","label":"Unity Integration","docId":"module-2-simulation/unity-integration","unlisted":false},{"type":"link","href":"/ai-robotics-book/docs/module-2-simulation/sensor-simulation","label":"Sensor Simulation","docId":"module-2-simulation/sensor-simulation","unlisted":false},{"type":"link","href":"/ai-robotics-book/docs/module-2-simulation/digital-twins","label":"Digital Twins","docId":"module-2-simulation/digital-twins","unlisted":false}],"href":"/ai-robotics-book/docs/category/module-2-simulation-environments"},{"type":"category","label":"Module 3: NVIDIA Isaac Platform","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-robotics-book/docs/module-3-isaac/isaac-sim-overview","label":"Isaac Sim Overview","docId":"module-3-isaac/isaac-sim-overview","unlisted":false},{"type":"link","href":"/ai-robotics-book/docs/module-3-isaac/isaac-ros","label":"Isaac ROS Integration","docId":"module-3-isaac/isaac-ros","unlisted":false},{"type":"link","href":"/ai-robotics-book/docs/module-3-isaac/vslam","label":"Visual SLAM","docId":"module-3-isaac/vslam","unlisted":false},{"type":"link","href":"/ai-robotics-book/docs/module-3-isaac/nav2-bipedal","label":"Nav2 for Bipedal Robots","docId":"module-3-isaac/nav2-bipedal","unlisted":false}],"href":"/ai-robotics-book/docs/category/module-3-nvidia-isaac-platform"},{"type":"category","label":"Module 4: Vision-Language-Action Models","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-robotics-book/docs/module-4-vla/voice-to-action","label":"Voice-to-Action Pipelines","docId":"module-4-vla/voice-to-action","unlisted":false},{"type":"link","href":"/ai-robotics-book/docs/module-4-vla/llm-planning","label":"LLM Cognitive Planning","docId":"module-4-vla/llm-planning","unlisted":false},{"type":"link","href":"/ai-robotics-book/docs/module-4-vla/multimodal-perception","label":"Multi-Modal Perception","docId":"module-4-vla/multimodal-perception","unlisted":false},{"type":"link","href":"/ai-robotics-book/docs/module-4-vla/end-to-end-vla","label":"End-to-End VLA Architectures","docId":"module-4-vla/end-to-end-vla","unlisted":false}],"href":"/ai-robotics-book/docs/category/module-4-vision-language-action-models"},{"type":"category","label":"Capstone Project","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-robotics-book/docs/capstone/","label":"Capstone Project","docId":"capstone/index","unlisted":false}],"href":"/ai-robotics-book/docs/category/capstone-project"},{"type":"category","label":"Resources","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-robotics-book/docs/resources/references","label":"References","docId":"resources/references","unlisted":false},{"type":"link","href":"/ai-robotics-book/docs/resources/glossary","label":"Glossary","docId":"resources/glossary","unlisted":false}],"href":"/ai-robotics-book/docs/category/resources"}]},"docs":{"capstone/index":{"id":"capstone/index","title":"Capstone Project","description":"Build a complete voice-controlled humanoid robot system","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Introduction","description":"An overview of Physical AI and Humanoid Robotics - the convergence of artificial intelligence with embodied systems","sidebar":"tutorialSidebar"},"module-1-ros2/architecture":{"id":"module-1-ros2/architecture","title":"ROS 2 Architecture","description":"Understanding the foundational architecture and design principles of ROS 2 for humanoid robotics","sidebar":"tutorialSidebar"},"module-1-ros2/nodes-topics-services":{"id":"module-1-ros2/nodes-topics-services","title":"Nodes, Topics, and Services","description":"Master the core communication patterns in ROS 2 - nodes, topics, services, and actions","sidebar":"tutorialSidebar"},"module-1-ros2/rclpy-integration":{"id":"module-1-ros2/rclpy-integration","title":"Python Development with rclpy","description":"Advanced Python development for ROS 2 including executors, callback groups, and lifecycle management","sidebar":"tutorialSidebar"},"module-1-ros2/urdf-humanoids":{"id":"module-1-ros2/urdf-humanoids","title":"URDF for Humanoid Robots","description":"Model humanoid robots using URDF and Xacro for simulation and control","sidebar":"tutorialSidebar"},"module-2-simulation/digital-twins":{"id":"module-2-simulation/digital-twins","title":"Digital Twins","description":"Creating synchronized digital representations of physical humanoid robots","sidebar":"tutorialSidebar"},"module-2-simulation/gazebo-basics":{"id":"module-2-simulation/gazebo-basics","title":"Gazebo Basics","description":"Introduction to Gazebo physics simulation for humanoid robotics development","sidebar":"tutorialSidebar"},"module-2-simulation/sensor-simulation":{"id":"module-2-simulation/sensor-simulation","title":"Sensor Simulation","description":"Simulating cameras, LiDAR, IMU, and force sensors for humanoid robots","sidebar":"tutorialSidebar"},"module-2-simulation/unity-integration":{"id":"module-2-simulation/unity-integration","title":"Unity Integration","description":"Integrating Unity game engine with ROS 2 for advanced robotics simulation","sidebar":"tutorialSidebar"},"module-3-isaac/isaac-ros":{"id":"module-3-isaac/isaac-ros","title":"Isaac ROS Integration","description":"Connecting NVIDIA Isaac Sim with ROS 2 for robot development","sidebar":"tutorialSidebar"},"module-3-isaac/isaac-sim-overview":{"id":"module-3-isaac/isaac-sim-overview","title":"Isaac Sim Overview","description":"Introduction to NVIDIA Isaac Sim for high-fidelity humanoid robot simulation","sidebar":"tutorialSidebar"},"module-3-isaac/nav2-bipedal":{"id":"module-3-isaac/nav2-bipedal","title":"Nav2 for Bipedal Robots","description":"Adapting ROS 2 Navigation Stack for humanoid robot locomotion","sidebar":"tutorialSidebar"},"module-3-isaac/vslam":{"id":"module-3-isaac/vslam","title":"Visual SLAM","description":"Implementing Visual Simultaneous Localization and Mapping for humanoid robots","sidebar":"tutorialSidebar"},"module-4-vla/end-to-end-vla":{"id":"module-4-vla/end-to-end-vla","title":"End-to-End VLA Architectures","description":"Building complete Vision-Language-Action models for humanoid control","sidebar":"tutorialSidebar"},"module-4-vla/llm-planning":{"id":"module-4-vla/llm-planning","title":"LLM Cognitive Planning","description":"Using Large Language Models for robot task planning and reasoning","sidebar":"tutorialSidebar"},"module-4-vla/multimodal-perception":{"id":"module-4-vla/multimodal-perception","title":"Multi-Modal Perception","description":"Integrating vision, language, and proprioception for humanoid awareness","sidebar":"tutorialSidebar"},"module-4-vla/voice-to-action":{"id":"module-4-vla/voice-to-action","title":"Voice-to-Action Pipelines","description":"Building speech interfaces for humanoid robot control","sidebar":"tutorialSidebar"},"resources/glossary":{"id":"resources/glossary","title":"Glossary","description":"Key terms and definitions for Physical AI and Humanoid Robotics","sidebar":"tutorialSidebar"},"resources/references":{"id":"resources/references","title":"References","description":"Bibliography and external resources for Physical AI and Humanoid Robotics","sidebar":"tutorialSidebar"}}}}')}}]);